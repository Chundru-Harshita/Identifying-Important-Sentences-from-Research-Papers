{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjdR/thN743t2tuzk6qahR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chundru-Harshita/Identifying-Important-Sentences-from-Research-Papers/blob/main/MiniProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPmmCyU1Pukv",
        "outputId": "2177e60a-d8b8-403f-b49b-931c881802e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4_2C8ZRQCUx",
        "outputId": "43700460-a717-4097-af45-ae2d0eae8184"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Data from Dataset\n",
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "df=pd.DataFrame(columns=['Line','Imp or Not'])\n",
        "\n",
        "s=\"/\"\n",
        "path = r'/content/drive/My Drive/data/test-data-master/test-data-master'\n",
        "for file in os.listdir(path):\n",
        "    path2 = path+s+file\n",
        "    imp=0\n",
        "    tot=0\n",
        "    domainSum=0\n",
        "    n=0\n",
        "    for file2 in os.listdir(path2):\n",
        "        path3 = path2+s+file2\n",
        "        c=0\n",
        "        l=[]\n",
        "        for file3 in os.listdir(path3):\n",
        "            path4 = path3+s+file3\n",
        "            if(len(file3)>11 and file3[-14:]==\"Stanza-out.txt\"):\n",
        "              df=pd.read_table(path4,header=None,names=['Lines'])\n",
        "              #print(df)\n",
        "\n",
        "              #Data Pre-processing and cleaning\n",
        "              port_stem = PorterStemmer()\n",
        "              def stemming(content):\n",
        "                  stemmed_content = re.sub('[^a-zA-Z]',' ',content)\n",
        "                  stemmed_content = stemmed_content.lower()\n",
        "                  stemmed_content = stemmed_content.split()\n",
        "                  stemmed_content = [port_stem.stem(word) for word in stemmed_content if not word in stopwords.words('english')]\n",
        "                  stemmed_content = ' '.join(stemmed_content)\n",
        "                  return stemmed_content\n",
        "              new_df=pd.DataFrame(columns=['Lines'])\n",
        "              new_df['Lines'] = df['Lines'].apply(stemming)\n",
        "\n",
        "              impsen=pd.read_table(path3+s+\"sentences.txt\",header=None,names=['impsent'])\n",
        "              implist=impsen['impsent'].tolist()\n",
        "              #implist\n",
        "\n",
        "              #Applying Tf-idf\n",
        "              X = new_df['Lines'].values\n",
        "              vectorizer = TfidfVectorizer()\n",
        "              vectorizer.fit(X)\n",
        "              X = vectorizer.transform(X)\n",
        "              #print(X)\n",
        "\n",
        "              #Selecting top ranked sentences\n",
        "              l={}\n",
        "              for i in range(0,len(df.index)):\n",
        "                if(len(X[i].data)!=0):\n",
        "                  l[i+1]=(sum(X[i].data)/len(X[i].data))\n",
        "                else:\n",
        "                  l[i+1]=0\n",
        "              sorted_dt = {key: value for key, value in sorted(l.items(), key=lambda item: item[1], reverse=True)}\n",
        "              ones=list(sorted_dt.values()).count(1)\n",
        "              impSen=list(sorted_dt.keys())[ones:ones+len(implist)]\n",
        "              impSen.sort()\n",
        "              implist.sort()\n",
        "              #print(impSen)\n",
        "              #print(implist)\n",
        "\n",
        "              #Calculating accuracies\n",
        "              count=0\n",
        "              for sent in impSen:\n",
        "                if sent in implist:\n",
        "                  count+=1\n",
        "              average=count/len(impSen)*100\n",
        "              domainSum+=average\n",
        "              n+=1\n",
        "    print(file,\" avg - \",domainSum/n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTnS9uQxRZ8b",
        "outputId": "29e42272-61a7-4470-e682-0be4c1efe631"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "natural_language_inference  avg -  5.276372322707839\n",
            "face_alignment  avg -  4.57872312462756\n",
            "coreference_resolution  avg -  5.5018658657096795\n",
            "face_detection  avg -  6.308804380464318\n",
            "entity_linking  avg -  7.305156074880434\n",
            "hypernym_discovery  avg -  3.349673202614379\n",
            "dependency_parsing  avg -  2.593630044610437\n",
            "data-to-text_generation  avg -  7.9602302459445315\n",
            "document_classification  avg -  5.976989795062791\n",
            "constituency_parsing  avg -  10.563695563695564\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "df=pd.DataFrame(columns=['Line','Imp or Not'])\n",
        "\n",
        "s=\"/\"\n",
        "path = r'/content/drive/My Drive/data/test-data-master/test-data-master'\n",
        "for file in os.listdir(path):\n",
        "    path2 = path+s+file\n",
        "    imp=0\n",
        "    tot=0\n",
        "    domainSum=0\n",
        "    n=0\n",
        "    for file2 in os.listdir(path2):\n",
        "      path3 = path2+s+file2\n",
        "      c=0\n",
        "      l=[]\n",
        "      print(\"\\nImportant points in\",file,\"-\",file2,\":\")\n",
        "      for file3 in os.listdir(path3):\n",
        "          if(file3==\"sentences.txt\"):\n",
        "              with open(path3+s+\"sentences.txt\", 'r') as fse:\n",
        "                  l=fse.readlines()\n",
        "      for file3 in os.listdir(path3):\n",
        "          sen=[]\n",
        "          if(len(file3)>11 and file3[-14:]==\"Stanza-out.txt\"):\n",
        "              with open(path3+s+file3, 'r') as fse:\n",
        "                  sen=fse.readlines()\n",
        "              for i in range(1,len(sen)+1):\n",
        "                  if i-1 in impSen:\n",
        "                    print(sen[i-1],end=\" \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tMiDttDYUNM",
        "outputId": "d9508d75-34e7-46c9-d471-05e1782aa9e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Important points in natural_language_inference - 8 :\n",
            "So far proposed deep learning approaches can be roughly categorized into two groups : sentence encoding - based models and matching encodingbased models .\n",
            " In this paper , we proposed a unified deep learning framework for recognizing textual entailment which dose not require any feature engineering , or external resources .\n",
            " In our work , we treated RTE task as a supervised three - way classification problem .\n",
            " Once the sentence vectors are generated .\n",
            " Inverting Premises Doubling Premises ( Zaremba and Sutskever , 2014 ) Doubling Hypothesis Differentiating Inputs ( Removing same words appeared in premises and hypothesis )\n",
            " While the hypothesises usually much shorter than premises , doubling hypothesis may absorb this difference and emphasize the meaning twice via this strategy .\n",
            " That is , each sentence have it s own color atla , the lightest color and the darkest color denoted the smallest attention weight the biggest value within the sentence , respectively .\n",
            " And more focused and accurate sentence representations were generated based on produced attention vectors .\n",
            " \n",
            "Important points in natural_language_inference - 7 :\n",
            "Percentage of time that the correct answer is contained in the top N most frequent entities in a given document .\n",
            " Such an approach requires a large training corpus of document - query - answer triples and until now such corpora have been limited to hundreds of examples and thus mostly of use only for testing .\n",
            " This results in a combined corpus of roughly 1 M data points .\n",
            " Therefore , following this procedure , the only remaining strategy for answering questions is to do so by exploiting the context presented with each question .\n",
            " In the case of multiple possible answers from a single rule , we randomly choose one .\n",
            " Here , we align the placeholder of the Cloze form question with each possible entity in the context document and calculate a distance measure between the question and the context around the aligned entity .\n",
            " When used for translation , Deep LSTMs have shown a remarkable ability to embed long sequences into a vector representation which contains enough information to generate a full translation in another language .\n",
            " Given the embedded document and query the network predicts which token in the document answers the query .\n",
            " The entity anonymisation and permutation aspect of the task presented here may end up levelling the playing field in that regard , favouring models capable of dealing with syntax rather than just semantics .\n",
            " With these considerations in mind , the experimental part of this paper is designed with a threefold aim .\n",
            " Our analysis indicates that the Attentive and Impatient Readers are able to propagate and integrate semantic information overlong distances .\n",
            " \n",
            "Important points in natural_language_inference - 6 :\n",
            "Motivated by dynamic routing ) , we propose a new self - attention mechanism for sentence embedding , namely Dynamic Self - Attention ( DSA ) .\n",
            " We achieve new state - of - the - art results in SNLI dataset , while showing comparative results in SST dataset .\n",
            " The weight vector v is static with respect to the input sequence X during inference .\n",
            " Dynamic Self - Attention ( DSA ) iteratively computes attention weights over words with the dynamic weight vector , which varies with inputs .\n",
            " In order to show how the dynamic weight vectors vary , we perform dimensionality reduction on them , z 1 at the ( r ?\n",
            " Thus , DSA adapts the dynamic weight vector with respect to each sentence .\n",
            " Entailment , Contradiction and Neutral .\n",
            " Directional self - attention network 91.1 85.6 2.4 587 600D\n",
            " \n",
            "Important points in natural_language_inference - 9 :\n",
            "Each Wikipedia page has a passage ( or long answer ) annotated on the page that answers the question and one or more short spans from the annotated passage containing the actual answer .\n",
            " On average we generate 30 instances per NQ example .\n",
            " This leads to a training set that has approximately 500,000 instances of 512 tokens each .\n",
            " Conclusion\n",
            " \n",
            "Important points in natural_language_inference - 25 :\n",
            "In multi-document MRC , depending on the way of combining the two components , document selection and extractive reading comprehension , there are two categories of approaches :\n",
            " The deep cascade model is designed to properly keep the balance between the effectiveness and efficiency .\n",
            " The module at each subsequent stage consumes the output from the previous stage , and further prunes the documents , paragraphs and answer spans given the question .\n",
            " uses an Adaboost style framework with two independent ranking functions in each stage , one for pruning the input ranked documents and the other for refining the rank order .\n",
            " To better combine different kinds of features , we adopt a scalable tree boosting method XGBoost for ranking , which is widely used to achieve state - of the - art results on many large - scale machine learning challenges .\n",
            " Multi- task Deep Attention Model\n",
            " Specifically , we first map each word into the vector space by concatenating its word embedding and CNN - based character embedding .\n",
            " D t = j ? tj u D j .\n",
            " Di = 0 .\n",
            " Paragraph Extraction\n",
            " In this way , we can restrain the joint training process so that the shared parameters will not change so much .\n",
            " \n",
            "Important points in natural_language_inference - 3 :\n",
            "proposed match LSTM to associate documents and questions and adapted the so - called pointer Network to determine the positions of the answer text spans .\n",
            " More specifically , we explore a tree - structured LSTM which extends the linear - chain long short - term memory ( LSTM ) ] to a recursive structure , which has the potential to capture long - distance interactions over the structures .\n",
            " And we use the pre-trained 300 - D Glo Ve vectors ( see the experiment section for details ) to initialize our word - level embedding .\n",
            " As detailed later , different question representation can be easily incorporated to this layer in addition to being used as a filter to find key information in the document based on the question .\n",
            " In general a TreeLSTM could perform semantic composition over given syntactic structures .\n",
            " Specifically , the input of a TreeLSTM node is used to configure four gates : the input gate it , output gate o t , and the two forget gates f\n",
            " For example , a \" when \" question seeks for different types of information ( i.e. , temporal information ) than those fora \" why \" question .\n",
            " We start from a simple way to first analyze the word frequency of all questions , and obtain top - 10 most frequent question types : what , how , who , when , which , where , why , be , whose , and whom , in which be stands for the questions beginning with different forms of the word be such as is , am , and are .\n",
            " We then compute a discriminative vector ?\n",
            " x between the input question with regard to the soft class - center vector :\n",
            " EM F1\n",
            " \n",
            "Important points in natural_language_inference - 5 :\n",
            "For example , by judging the dissimilar parts , we can easily identify that E 3 and E 5 share the similar meaning \" The study is about salmon \" , because \" sockeye \" belongs to the salmon family , and \" flounder \" does not .\n",
            " The third challenge did not get much\n",
            " E4\n",
            " With those pre-trained embeddings , we transform Sand T into sentence matrixes S = [ s 1 , ... , s i , ... , s m ] and T = [t 1 , ... , t j , ... , tn ] , where s i and t j are d-dimension vectors of the corresponding words , and m and n are sentence length of Sand T respectively .\n",
            " In this section , we describe details of each phase .\n",
            " ( 1 ) .\n",
            " The local -w function takes a compromise between global and max , where w indicates the size of the window to consider centered at k ( the most similar word position ) .\n",
            " The intention off decomp is to decompose a word vector s j based on its semantic matching vector ?\n",
            " For the convolution operation , we define a list of filters {w o }.\n",
            " The shape of each filter is d h , where dis the dimension of word vectors and h is the window size .\n",
            " Therefore , we use the local - 3 function in the following experiments .\n",
            " \n",
            "Important points in natural_language_inference - 31 :\n",
            "In this paper , we present a MANN named SAM ( sparse access memory ) .\n",
            " This Sparse Differentiable Neural Computer ( SDNC ) is over 400 faster than the canonical dense variant fora memory size of 2,000 slots , and achieves the best reported result in the Babi tasks without supervising the memory access .\n",
            " RN is a vector of weights with non-negative entries that sum to one .\n",
            " Read\n",
            " If there are several words that minimize\n",
            " The first definition is a time - discounted sum of write weights U ( 1 )\n",
            " Controller\n",
            " The full control flow is illustrated in .\n",
            " 3 . Priority Sort : Given 20 random keys and priority values , return the top 16 keys in descending order of priority .\n",
            " We chose these tasks because the NTM is known to perform well on them .\n",
            " Again , we used an exponential curriculum , doubling the number of additional characters provided to the model whenever the cost was reduced under a threshold .\n",
            " \n",
            "Important points in natural_language_inference - 27 :\n",
            "Different from the canonical way of appending a thin layer after BERT structure according to , we innovatively employed a weighted sum of BERT layer outputs , with locked BERT parameters .\n",
            " In this section , we propose the neural model , SDNet , for the conversational question answering task , which is formulated as follows .\n",
            " Encoding layer encodes each token in passage and question into a fixed - length vector , which includes both word embeddings and contextualized embeddings .\n",
            " ij based on two sets of vectors A and B , and use that to linearly combine vector set C .\n",
            " Self Attention on Context .\n",
            " OUTPUT LAYER\n",
            " where WY and w Y are parametrized matrix and vector , respectively .\n",
            " where i s k and i e k are the ground - truth span start and end position for the k - th question .\n",
            " We experimented the effect of N and show the result in .\n",
            " Excluding dialogue history ( N = 0 ) can reduce the F 1 score by as\n",
            " \n",
            "Important points in natural_language_inference - 26 :\n",
            "Question\n",
            " There are two kinds of approaches to model the answerability of a question .\n",
            " Since these three sub - tasks are highly related , we regard the MRC with unanswerable questions as a multi-task learning problem ( Caruana 1997 ) by sharing some meta-knowledge .\n",
            " Universal Node\n",
            " Take the first level as an example .\n",
            " R ( m+ 1 ) ( n + 1 ) ; W 1 and W 2 are learnable parameters .\n",
            " Then we concatenate the original embedded representation V and H A for better representation of the fused information of passage , universal node , and question .\n",
            " Now\n",
            " The no-answer pointer and plausible answer pointer are removed attest phase .\n",
            " ( iii ) Answer Verifier\n",
            " We use 100 - dim Glove pretrained word embeddings and 1024 - dim Elmo embeddings .\n",
            " \n",
            "Important points in natural_language_inference - 4 :\n",
            "To overcome this issue , we propose two novel strategies that improve the memory - handling capability while mitigating the information distortion .\n",
            " The main contributions of this work include the following :\n",
            " We modified the recently proposed memory controller by using our new encoder block and layer - wise residual connections .\n",
            " Then we concatenate each output vector rt to the projected input pt to obtain gt = [ r t ; pt ] ?\n",
            " C q is calculate as\n",
            " Then document - attended question vector is obtained b ?\n",
            " We perform extensive experiments with well - known benchmarks such as TriviaQA , QUASAR - T , and SQuAD , as summarized in .\n",
            " QA is composed of question - answer pairs obtained from 14 trivia and quiz - league websites , along with the documents collected later that are likely to contain the answer from either web search or Wikipedia .\n",
            " Due to the relatively short document length in SQuAD compared to TriviaQA and QUASAR - T , our model without DEBS performs worse than the baseline ' BiDAF + Self Attention + ELMo. '\n",
            " However , after applying DEBS , our model outperforms\n",
            " Related Work\n",
            " \n",
            "Important points in natural_language_inference - 29 :\n",
            "This perspective leads to a different attention - based architecture containing two sequential phases , question - aware passage representation phase and evidence propagation phase. , RNET , MReader , and PhaseCond ( our proposed model ) .\n",
            " 1 ) we proposed a phase conductor for attention models containing multiple phases , each with a stack of attention layers producing passage representations and a stack of inner or outer fusion layers regulating the information flow , 2 ) we present an improved attention function for question - passage attention based on two kinds of encoders : an independent question encoder and a weight - sharing encoder jointly considering the question and the passage , as opposed to most previous works which only using the same encoder for one attention model , and 3 ) we provide both detailed qualitative analysis and visualized examples showing the dynamic changes through multi-layered attention models .\n",
            " For each self - attention layer , we configure an inner fusion layer to obtain a gated representation that is learned to decide how much of the current output is fused by the input from the previous layer detailed in Section 2.3.1 .\n",
            " OUTER FUSION LAYERS\n",
            " The output ht i , whose dimensions are the same as its input vector B ti , is then sent to the next layer of self - attention model as input to calculate Eq 9 and Eq 10 .\n",
            " The memory - based answer pointer network contains multiple hops .\n",
            " Following , we also use question type ( what , how , who , when , which , where , why , be , and other ) features where each type is represented by a trainable embedding .\n",
            " MAIN RESULTS OF MODEL COMPARISON\n",
            " shows that repeatedly representing a passage word regarding the same question representation can make the passage embedding become closer to the original question representation .\n",
            " Third , when comparing and , we observed that the color is diluted for most of the weights in the second layer of self - attention phase , meanwhile a small portion of weights is strengthened , suggesting that information propagation is converging .\n",
            " \n",
            "Important points in natural_language_inference - 30 :\n",
            "We start by introducing a convolution network in Section 2 as the basic architecture for sentence modeling , and how it is related to existing sentence models .\n",
            " As inmost convolutional models , we use convolution units with a local \" receptive field \" and shared weights , but we design a large feature map to adequately model the rich structures in the composition of words .\n",
            " ?\n",
            " Instead , it takes multiple choices of composition via a large feature map ( encoded in w ( , f ) for different f ) , and leaves the choices to the pooling afterwards to pick the more appropriate segments ( in every adjacent two ) for each composition .\n",
            " Clearly the 1D convolution preserves the location information about both segments .\n",
            " ( 5 )\n",
            " Our experiments show that when ARC - II is trained on the ( S X , S Y ,S Y ) triples whereS Y randomly shuffles the words in S Y , it consistently gains some ability of finding the correct S Y in the usual contrastive negative sampling setting , which however does not happen with ARC - I.\n",
            " As a result , the output for each filter f , denoted z\n",
            " The matching score of two short - texts are calculated with an MLP with the embedding of the two documents as input ; DEEPMATCH : We take the matching model in and train it on our datasets with 3 hidden layers and 1,000 hidden nodes in the first hidden layer ; URAE+ MLP :\n",
            " We use the Unfolding Recursive Autoencoder to get a 100 dimensional vector representation of each sentence , and put an MLP on the top as in WORDEMBED ; SENNA + MLP / SIM :\n",
            " When dealing with language objects , most methods still focus on seeking vectorial representations in a common latent space , and calculating the matching score with inner product .\n",
            " \n",
            "Important points in natural_language_inference - 28 :\n",
            "In this paper , we investigate this problem through a simple story understanding scenario , in which the agent is given a sequence of textual statements and events , and then given another series of statements about the final state of the world .\n",
            " Each cell is associated with its own \" processor \" , a simple gated recurrent network that may update the cell value given an input .\n",
            " Our experiments also indicate that the model indeed maintains an internal representation of the simplified world in which the stories take place , and that the model does not limit itself to storing the aspects of the world required to answer a specific question .\n",
            " The function ?\n",
            " For example , given a list of named entities ( which could be produced by a standard tagger ) , we could make the model have a separate memory slot for each entity .\n",
            " Now , consider that the model reads the following two sentences , and the desired behavior of the gating function and update function at each memory as they are seen :\n",
            " The EntNet is related to gated recurrent models such as the LSTM and GRU , which also use gates to fix or modify the information stored in the hidden state .\n",
            " On the other hand , the DNC has a relatively sophisticated controller network ( such as an LSTM ) which reads an input and outputs a number of interface vectors ( such as keys and weightings ) which are then combined via a softmax to read from and write to the external memory matrix .\n",
            " It also means the model can be trained less expensively .\n",
            " To study this , we trained an EntNet on variable length sequences between 1 and 20 , and evaluated it on different length sequences longer than 20 .\n",
            " Key embeddings were tied to the embeddings of the candidate words , resulting in 10 hidden blocks , one per member of C. Due to the weight tying , we did not need a decoder matrix and used the distribution over candidates to directly produce a prediction , as described in Section 3 .\n",
            " \n",
            "Important points in natural_language_inference - 21 :\n",
            "To sum up , the main contributions of our work are listed as follows .\n",
            " The experimental results and analysis will be given in Section 5 and Section 6 .\n",
            " D , Q , A\n",
            " In their studies , they have found that the answering of verbs and prepositions are relatively less dependent on the content of document , and the humans can even do preposi-tion blank - filling without the presence of the document .\n",
            " Different from , instead of using naive heuristics ( such as summing or averaging ) to combine these individual attentions into a final attention , we introduce another attention mechanism to automatically decide the importance of each individual attention .\n",
            " We denote ?( t ) ?\n",
            " Final Predictions\n",
            " CNN News 118,497 53,063 53,185\n",
            " CNN\n",
            " News\n",
            " The following analyses are carried out on CBTest NE dataset .\n",
            " \n",
            "Important points in natural_language_inference - 17 :\n",
            "This dataset is designed to highlight common challenges , such as the use of world knowledge and logical operators , that we expect models must handle to robustly solve the tasks . :\n",
            " Our best multi-task model makes use of ELMo , a recently proposed pre-training technique .\n",
            " RELATED WORK\n",
            " Unless otherwise mentioned , tasks are evaluated on accuracy and are balanced across classes .\n",
            " INFERENCE TASKS\n",
            " The premise sentences are gathered from ten different sources , including transcribed speech , fiction , and government reports .\n",
            " The task is to predict if the sentence with the : Examples from the diagnostic set .\n",
            " We use a small evaluation set consisting of new examples derived from fiction books 5 that was shared privately by the authors of the original corpus .\n",
            " The models respectively get near - chance accuracies of 32.7 % and 36.4 % on our diagnostic data , showing that the data does not suffer from such artifacts .\n",
            " To establish human baseline performance on the diagnostic set , we have six NLP researchers annotate 50 sentence pairs ( 100 entailment examples ) randomly sampled from the diagnostic set .\n",
            " We find that multi-task training yields better overall scores over single - task training amongst models using attention or ELMo .\n",
            " \n",
            "Important points in natural_language_inference - 18 :\n",
            "Motivated by these ideas , we incorporate an \" adaptive initialization \" for neural network training ( see section 2 for details ) , where we use cyclical batch size schedules to control the noise ( or temperature ) of SGD .\n",
            " The CBS schedule leads to multiple perplexity improvement ( up to 7.91 ) in language modeling and minor improvements in natural language inference and image classification .\n",
            " Building off this idea , explored anew strategy known as MSRA to keep the variance constant for all convolutional layers .\n",
            " Additionally , the high dimensionality of the NN 's parameter space renders various approaches such as adaptive priors intractable ( e.g. adaptive MCMC algorithms ) .\n",
            " Notice that almost all CBS schedules outperform the baseline schedule .\n",
            " The peaks occur during decreases in batch size , i.e. , increases in noise scale , which could help to escape sub-optimal local minima , and the troughs occur during increases in batch size , i.e. , decreases with noise scale .\n",
            " We observe that CBS achieves similar performance to the baseline .\n",
            " This outperforms other schedules shown in .\n",
            " Tasks : We train networks to perform the following supervised learning tasks :\n",
            " Image classification .\n",
            " We set a backpropagation - through - time limit of 35 steps and clip the max gradient norm at 0.5 .\n",
            " \n",
            "Important points in natural_language_inference - 23 :\n",
            "Some models build the interaction at different granularity ( word , phrase and sentence level ) , such as ARC - II , MultiGranCNN , Multi - Perspective CNN , , MatchPyramid .\n",
            " More concretely , DF - LSTMs consist of two interconnected conditional LSTMs , each of which models apiece of text under the influence of another .\n",
            " 3 .\n",
            " Intuitively , the forget gate controls the amount of which each unit of the memory cell is erased , the input gate controls how much each unit is updated , and the output gate controls the exposure of the internal memory state .\n",
            " We denote a read vector from external memories as r i , j ?\n",
            " More concretely , each scalar a i , j , k in attention distribution a i , j can be obtained : where M i , j , k ?\n",
            " What is different is the dependency relations between the current state and history states .\n",
            " Training\n",
            " Attention LSTMs : Two sequences are encoded by LSTMs with attention mechanism , proposed by .\n",
            " Word - by - word Attention LSTMs : An improved strategy of attention LSTMs , which introduces word - by - word attention mechanism and is proposed by . :\n",
            " For example , our model gives a wrong prediction of the sentence pair \" A golden retriever nurses puppies / Puppies next to their mother \" , whose ground truth is entailment .\n",
            " \n",
            "Important points in natural_language_inference - 24 :\n",
            "In sharp contrast , methods that use KBs or information retrieval over documents have to employ search as an integral part of the solution .\n",
            " In particular , we show that performance is improved across all datasets through the use of multitask learning and distant supervision compared to single task training .\n",
            " An objective of this paper is to test how such new methods can perform in an open - domain QA framework .\n",
            " A simple inverted index lookup followed by term vector model scoring performs quite well on this task for many question types , compared to the built - in ElasticSearch based Wikipedia Search API .\n",
            " , and ? ( ) is a single dense layer with ReLU nonlinearity .\n",
            " The question encoding is simpler , as we only apply another recurrent neural network on top of the word embeddings of q i and combine the resulting hidden units into one single vector : {q 1 , . . . , q l } ? q.\n",
            " ( 1 ) Wikipedia that serves as our knowledge source for finding answers , ( 2 ) the SQuAD dataset which is our main resource to train Document Reader and ( 3 ) three more QA datasets ( Curated TREC , We-bQuestions and WikiMovies ) that in addition to SQuAD , are used to test the open - domain QA abilities of our full system , and to evaluate the ability of our model to learn from multitask learning and distant supervision .\n",
            " 3\n",
            " How many provinces did the Ottoman empire contain in the 17th century ?\n",
            " A : 32\n",
            " Additionally , we think that our model is conceptually simpler than most of the existing systems .\n",
            " \n",
            "Important points in natural_language_inference - 2 :\n",
            "R.\n",
            " 1 ) the lexicon encoding layer computes word representations ; 2 ) the contextual encoding layer modifies these representations in context ; 3 ) the memory generation layer gathers all information from the premise and hypothesis and forms a\n",
            " R dm and E h ?\n",
            " Pr t is a probability distribution overall the relations , { 1 , . . . , | R |}.\n",
            " The mini - batch size is set to 32 .\n",
            " One main question which we would like to address is whether the multi-step inference help on NLI .\n",
            " On SciTail dataset , SAN even outperforms GPT .\n",
            " 7 .\n",
            " \n",
            "Important points in natural_language_inference - 19 :\n",
            "Neutral ( irrelevant ) .\n",
            " Although they have achieved high performance , they may either fail to fully make use of the syntactical information in sentences or be difficult to train due to the long propagation path .\n",
            " Finally , two sentences ' information is combined by several heuristic matching layers , including concatenation , element - wise product and difference ; they are effective in capturing relationships between two sentences , but remain low complexity .\n",
            " Our Approach\n",
            " Therefore , they may vary in size and length .\n",
            " Then we add a fully - connected hidden layer to further mix the information in a sentence .\n",
            " These matching layers are further concatenated , given by\n",
            " R 4nc is the output of the matching layer .\n",
            " compares different heuristics of matching .\n",
            " We first analyze each heuristic separately : using element - wise product alone is significantly worse than concatenation or element - wise difference ; the latter two are comparable to each other .\n",
            " \n",
            "Important points in natural_language_inference - 16 :\n",
            "The complexity of such variation comes from the fact that the level of context understanding needed in order to correctly predict different types of words varies greatly .\n",
            " An important property of cloze - style questions is that a large amount of such questions can be automatically generated from real world documents .\n",
            " Formal Task Description\n",
            " Basic statistics about the CNN , Daily Mail and CBT datasets are summarized in .\n",
            " We call this mechanism pointer sum attention since we use attention as a pointer over discrete tokens in the context document and then we directly sum the word 's attention across all the occurrences .\n",
            " A high level structure of our model is shown in .\n",
            " The query encoder g is implemented by another bidirectional GRU network .\n",
            " During training we jointly optimize parameters off , g and e.\n",
            " One difference is that the attention weights are computed with a bilinear term instead of simple dot -product , that is s i ? exp ( f i ( d ) W g ( q ) ) .\n",
            " The document embedding r is computed using a weighted sum as in the Attentive Reader , r = i s if i ( d ) .\n",
            " For each batch of the CNN and Daily Mail datasets we randomly reshuffled the assignment of named entities to the corresponding word embedding vectors to match the procedure proposed in .\n",
            " \n",
            "Important points in natural_language_inference - 22 :\n",
            "For example , if question1 and question2 are paraphrases , and question2 and question3 are paraphrases , we can infer that question1 and question3 are also paraphrases .\n",
            " We evaluated our proposed method on various QA datasets and the experimental results show the effectiveness and superiority of our method .\n",
            " For the paraphrase identification ( PI ) task , NLSM is utilized to determine whether two sentences are paraphrases or not .\n",
            " As shows , we employ a multi-task learning method to simultaneously train a sentence matching model and a sentence intent classification model by sharing the sentence encoder between two tasks .\n",
            " Forward directional GRU process inputs sequence from left to right , backward directional GRU on the contrary .\n",
            " In GRU cell , context information of tth hidden unit is carried over by the last hidden unit state ?\n",
            " After that , we use a highway connection to connect input and output :\n",
            " It uses a multi - layer perceptron with two layers and uses activation function ReLU , as follows :\n",
            " shows a brief description of these datasets .\n",
            " The overlap rate is a ratio of the number of common words between two sentences in a sample to the average number of them .\n",
            " TCS dataset :\n",
            " \n",
            "Important points in natural_language_inference - 20 :\n",
            "Furthermore , we propose different variants of node composition function and attention over tree for our NTI models .\n",
            " Dotted lines indicate attention over premise - indexed tree .\n",
            " Among them , it is not efficient at memorizing long or distant sequence .\n",
            " Each input word token wt is represented by it s word embedding x t ?\n",
            " R k .\n",
            " R k2 resulted by concatenating the child node representations h l t , hr t and the third input vector q , ANF is defined as\n",
            " the outer product .\n",
            " This imposes practical difficulties when processing long sequences .\n",
            " We analyzed the effects of the padding size and found out that it has no influence on the performance ( see Appendix 5.3 ) .\n",
            " The size of hidden units of the NTI modules were set to 300 .\n",
            " The hyper - parameters are the same as the previous model .\n",
            " \n",
            "Important points in natural_language_inference - 12 :\n",
            "Generalization updates old memories given the new input and output feature map finds relevant information from the memory .\n",
            " The design philosophy behind it is to directly capture the supporting relation between the sentences through the multi - layer perceptron ( MLP ) .\n",
            " When n is small , the cost of learning relation is reduced by n times compared to MemNN based models , which enables more data - efficient learning .\n",
            " ti between 0 and 1 .\n",
            " RELATION NETWORK\n",
            " and f ? .\n",
            " Each task requires different methods to infer the answer .\n",
            " Each task is regarded as success when the accuracy is greater than 95 % .\n",
            " Sometimes both g 1 ?\n",
            " and g 2 ?\n",
            " inner product was used in MemN2N , inner product with gate was used in GMe m N2N , and inner product and absolute difference with two embedding matrices was used in DMN and DMN + .\n",
            " \n",
            "Important points in natural_language_inference - 0 :\n",
            "On synthetic data specifically constructed to test coreferencebased reasoning , C - GRUs lead to a large improvement over regular GRUs .\n",
            " presented a generative model for jointly predicting the next word in the text and its gold - standard coreference annotation .\n",
            " Our resulting layer is structurally similar to Graph LSTMs , with an additional attention mechanism over the graph edges .\n",
            " which decompose the hidden states into a sequential and a coreferent component , respectively .\n",
            " Avg Max # failed :\n",
            " A task is considered failed if its Max performance is < 0.95 .\n",
            " Comparing to the QRN baseline , we found that C - GRU was significantly worse on task 15 ( basic deduction ) .\n",
            " On the other hand , in ( left ) we show how the performance of GA w / C - GRU varies as we remove gold - standard mentions from coreference clusters , or if we replace them with random mentions ( GA w / random - GRU ) .\n",
            " The proposed GA w / C - GRU layer sets a new state - of the - art on this dataset .\n",
            " Conclusion\n",
            " \n",
            "Important points in natural_language_inference - 14 :\n",
            "The continuous representation is then processed via multiple hops to output a .\n",
            " Suppose we are given an input set x 1 , .. , xi to be stored in memory .\n",
            " The response vector from the memory o is then a sum over the transformed inputs c i , weighted by the probability vector from the input :\n",
            " Emitting an internal output corresponds to considering a memory , and emitting an external output corresponds to predicting a label .\n",
            " There are also differences in the architecture of the small network used to score the memories compared to our scoring approach ; we use a simple linear layer , whereas they use a more sophisticated gated architecture .\n",
            " Bengio et al.\n",
            " Two versions of the data are used , one that has 1000 training problems per task and a second larger one with 10,000 per task .\n",
            " In our experiments we explore two different representations for the sentences .\n",
            " Note that MemN2N has ? 1.5x more parameters than RNNs with the same number of hidden units , while LSTM has ? 4x more parameters .\n",
            " We also vary the number of hops and memory size of our MemN2N , showing the contribution of both to performance ; note in particular that increasing the number of hops helps .\n",
            " This is split into 93.3M / 5.7M / 1M character train / validation / test sets .\n",
            " \n",
            "Important points in natural_language_inference - 15 :\n",
            "In this paper we enrich neural - network - based NLI models with external knowledge in coattention , local inference collection , and inference composition components .\n",
            " These models mainly fall into two types of approaches : sentence - encoding - based models and models using also inter-sentence attention .\n",
            " proposed an enhanced sequential inference model ( ESIM ) , which is one of the best models so far and is used as one of our baselines in this paper .\n",
            " Same as much previous work , we encode the premise and the hypothesis with bidirectional LSTMs ( BiLSTMs ) .\n",
            " Through comparing a s i and ac i , in addition to their relation from external knowledge , we can obtain word - level inference information for each word .\n",
            " where a heuristic matching trick with difference and element - wise product is used .\n",
            " In this component , we introduce knowledgeenriched inference composition .\n",
            " Our inference model converts the output hidden vectors of BiLSTMs to the fixed - length vector with pooling operations and puts it into the final classifier to determine the overall inference class .\n",
            " In our work here , we also capture the relation between any two words in WordNet through relation embedding .\n",
            " Specifically , we employed TransE , a widely used graph embedding methods , to capture relation embedding between any two words .\n",
            " To compare the effects of external knowledge under different training data scales , we ran-Model Test LSTM Att. 83.5 DF - LSTMs 84.6 TC - LSTMs 85.1 Match- LSTM 86.1 LSTMN 86.3 Decomposable Att. 86.8 NTI 87.3 Re-read LSTM 87.5 BiMPM 87.5 DIIN 88.0 BCN + CoVe 88.1 CAFE 88.5\n",
            " \n",
            "Important points in natural_language_inference - 1 :\n",
            "The second phase is based on an encoder - decoder architecture , which comprehends the identified regions of text and generates the answer by using a residual self - attention network as encoder and a RNNbased sequence generator along with a pointer network as the decoder .\n",
            " We evaluate our model using one of the challenging RC datasets , called ' NarrativeQA ' , which was released recently by .\n",
            " These wordbased embeddings are concatenated with their corresponding char embeddings .\n",
            " where ?\n",
            " Training\n",
            " Estimating ?\n",
            " It is shown that any number will reduce the variance , here we used the mean of the mini-batch reward b as our baseline .\n",
            " Dataset\n",
            " In addition , the low performance of Baseline 2 indicates that just selecting multiple sentences is not enough , they should also be related to each other .\n",
            " This result points out that the self - attention mechanism in the Context zoom layer is an important component to identify related relevant sentences .\n",
            " \n",
            "Important points in natural_language_inference - 11 :\n",
            "The model consists of a coattentive encoder that captures the interactions between the question and the document , as well as a dynamic pointing decoder that alternates between estimating the start and end of the answer span .\n",
            " What plants create most electric power ?\n",
            " We define the document encoding matrix as D = [ d 1 . . . d m d ? ]\n",
            " DYNAMIC POINTING DECODER\n",
            " We now describe the HMN model : where r ?\n",
            " ( 1 ) t is the output of the first maxout layer with parameters W ( 1 ) ?\n",
            " Statistical QA\n",
            " proposed a competitive statistical baseline using a variety of carefully crafted lexical , syntactic , and word order features .\n",
            " The offical SQuAD evaluation is hosted on CodaLab 2 .\n",
            " The training and development sets are publicly available while the test set is withheld .\n",
            " Performance across question type\n",
            " \n",
            "Important points in natural_language_inference - 13 :\n",
            "The task is then to find the answer that \" fills in the blank \" .\n",
            " Hypothesize ?\n",
            " The Reasoner forms hypotheses by inserting the candidate answers into the question , then estimates the concordance of each hypothesis with each sentence in the supporting text .\n",
            " Children 's Book Test\n",
            " The end - toend EpiReader model , combining the Extractor and Reasoner modules , is depicted in .\n",
            " The Extractor\n",
            " We selected the GRU because it is computationally simpler than Long Short - Term Memory ( Hochreiter and Schmidhuber , 1997 ) , while still avoiding the problem of vanishing / exploding gradients often encountered when training recurrent networks .\n",
            " We concatenate these to yield a vector f ( t i ) ?\n",
            " R Dm , where m is the convolutional filter width .\n",
            " We add a bias term and apply a nonlinearity ( we use a ReLU ) following the convolution .\n",
            " Intuitively , this loss says that we want the end - to - end probability ?\n",
            " \n",
            "Important points in natural_language_inference - 10 :\n",
            "Wrong Ending : Bob stopped talking to those friends .\n",
            " This paper is structured as follows : we will discuss previous approaches to the problem and how they compare to our approach , describe our model and the experiments we ran in detail , and finally discuss reasons for our model 's superior performance and why ignoring the first three sentences of the story produces better accuracy .\n",
            " Like us , they train their model on the validation set , but their approach relies more heavily on feature engineering .\n",
            " During inference time , we make a forward pass with each of the two possible endings , and select the ending that has a higher probability of being the ' right ' ending .\n",
            " This approach is far simpler than previous approaches in the literature ; it requires no feature engineering , nor intricate neural network architecture , and achieves close to state - of - the - art accuracy .\n",
            " While the BookCorpus and ROCStories draw from different distributions , it is possible that skip - thought vectors implicitly encode a general notion of typical story continuation .\n",
            " We have shown a simple yet effective neural model that achieves high accuracy on the Cloze Test , which is within 1.1 % of the state - of - the - art approach that relies on feature engineering .\n",
            " \n",
            "Important points in face_alignment - 9 :\n",
            "of which is the degraded performance caused by ' semantic ambiguity ' .\n",
            " In this paper , we propose a novel Semantic Alignment method which reduces the ' semantic ambiguity ' intrinsi-cally .\n",
            " We conduct experiments on 300W , AFLW and 300 - VW databases and achieve the state of the art performance .\n",
            " In this work , we find the semantic ambiguity can happen on any facial points , but mainly on those weak semantic facial points .\n",
            " In this section , we detail our methodology .\n",
            " Then we model the prior model and likelihood in Section 4.2 and 4.3 , respectively .\n",
            " As a result , the annotations , in particular those of weak semantic landmarks , contain random noises and are inconsistent among faces .\n",
            " where o is the observation of ? , for example , the annotation can be seen as an observation of ?\n",
            " k , the prior Eq. indicates that the latent ' real ' landmark is close to the observed landmark o k .\n",
            " Therefore , we reduce the search space of y k to a small patch centered on o k .\n",
            " Experiments\n",
            " \n",
            "Important points in face_alignment - 5 :\n",
            "Each facial landmark is strongly associated with a well - defined facial boundary , e.g. , eyelid and nose bridge .\n",
            " In this work , we represent facial structure using 13 boundary lines .\n",
            " We used stacked hourglass structure to estimate facial boundary heatmap and model the structure between facial boundaries through message passing to increase its robustness to occlusion .\n",
            " Yang et al .\n",
            " On the contrary , boundary lines are not necessary to form a closed loop , which is more flexible in representing geometric structure .\n",
            " It is composed of three closely related components :\n",
            " K subsets\n",
            " Then a binary boundary map Bi , the same size as I , is formed by setting only points on the boundary line to be 1 , others\n",
            " Following previous work in face alignment and human pose , we use stacked hourglass as the baseline of boundary heatmap estimator .\n",
            " Mean square error ( MSE ) between generated and groundtruth boundary heatmaps is optimized .\n",
            " The following pseudo - code shows the training process of the whole methods .\n",
            " \n",
            "Important points in face_alignment - 6 :\n",
            "In this paper we present the 3 DDE ( 3D Deeply - initialized Ensemble ) regressor , a robust and efficient face alignment algorithm based on a coarse - to - fine cascade of ERTs .\n",
            " Here we refine and extend it in several ways .\n",
            " Face alignment has been a topic of intense research for more than twenty years .\n",
            " There is also an increasing number of works based on 3D face models .\n",
            " R L3 be the 3D coordinates of the L landmarks on the 3D face model , x ?\n",
            " We produce sub - sets of correspondences ( x s , X s ) from the distinct landmarks shown in , estimate the 3D face model pose ( R , t ) with softPOSIT and evaluate the goodness of each estimation as the sum of landmarks probabilities ,\n",
            " { 0 ,\n",
            " ) and x j are the coordinates of the landmarks estimated in j-th stage .\n",
            " However , ours are defined on the probability maps , P ( I ) , instead of the image , I .\n",
            " We let the training algorithm select the most informative landmark and pair of pixels in each iteration .\n",
            " We also output the mean visibility of the samples reaching the tree leaf .\n",
            " \n",
            "Important points in face_alignment - 18 :\n",
            "A landmark heatmap is an image with high intensity values around landmark locations where intensity decreases with the distance from the nearest landmark .\n",
            " This improvement allows our method to make use of the entire image of a face , instead of local patches , and avoid falling into local minima .\n",
            " Tt which is used to warp the input image to a canonical pose .\n",
            " MIX also uses SIFT for feature extraction , while regression is performed using a mixture of experts , where each expert is specialized in a certain part of the space of face shapes .\n",
            " St so that St is close to the canonical shape S 0 .\n",
            " The details of the Transform Estimation , Image Transform and Landmark Transforms layer are described in subsection 3.2 .\n",
            " Normalization to canonical shape\n",
            " As input the layer takes the output of the current stage St . Once T t + 1 is estimated the Image Transform and the Landmark Transform layers transform the image I and landmarks\n",
            " where S * is a vector of ground truth landmark locations , T t is the transform that normalizes the input image and shape for stage t and d ipd is the distance between the pupils of S * .\n",
            " The use of this error is motivated by the fact that it is afar more common benchmark for face alignment methods than the Sum of Squared Errors .\n",
            " For evaluating our method on the test datasets we use three metrics : the mean error , the area under the cumulative error distribution curve ( AUC ? ) and the failure rate .\n",
            " \n",
            "Important points in face_alignment - 8 :\n",
            "3 D face reconstruction from a single 2D image is a challenging problem with broad applications .\n",
            " Introduction\n",
            " Even some 3 D face datasets , like 300W - LP , are publicly available , they generally lack diversity in face appearance , expression , occlusions and environment conditions , limiting the generalization performance of resulted 3 D face regression models .\n",
            " Related work\n",
            " directly learn the correspondence between a 2 D face image and a 3D template via a deep CNN , while only visible face - region is considered .\n",
            " Unfortunately , currently face datasets with 3D annotations are very limited .\n",
            " The rendering of a 3 D face shape is thus formulated as :\n",
            " R 3N 10 is the expression principle basis and ? exp ?\n",
            " The L3d- con is employed to constrain landmarks matching in 3D space during the cycle training .\n",
            " Here i indexes the landmark .\n",
            " where x 2 d are the input 2 D facial landmarks , andx 2 d are the landmarks output from Q ( F ( X ) ) . :\n",
            " \n",
            "Important points in face_alignment - 2 :\n",
            "DeCaFA is composed of several stages that each produce landmark - wise attention maps , relatively to heterogeneous annotation markups .\n",
            " We show experimentally that DeCaFA significantly outperforms existing approaches on multiple datasets , inluding the recent WFLW database .\n",
            " MDM improves the feature extraction process by sharing the convolutional layer among all steps of the cascade that are performed through a recurrent neural network .\n",
            " Section 3.2 explains how several transfer layers can be chained to produce such attention maps , relatively to K landmark prediction tasks .\n",
            " we chain the landmark - wise attention maps in an decreasing order of the number of landmarks to predict ) .\n",
            " We have :\n",
            " Within an end - to - end fully - convolutional deep network , an analogous statement would be that the i + 1 th stage shall use a local embedding F 2 that is calculated using information from the original image I highlighted by landmark - wise attention maps ?\n",
            " Where denotes the Hadamard product .\n",
            " In our experiments , we train our models using the train partition that contains 162770 images from 8 k identities .\n",
            " The test partition contains 19962 instances from 1 k identities that are not seen in the train set .\n",
            " 10M ) compared to state - of - theart approaches , and is also modular : attest time , DeCaFA allows to find a good trade - of between speed and accuracy ( by evaluating only a fraction of the cascade ) , as well as to predict various numbers of landmarks .\n",
            " \n",
            "Important points in face_alignment - 16 :\n",
            "Zhou et al. , and Zhang et al .\n",
            " shows that landmarks on the right side of face are almost invisible .\n",
            " Moreover , to decrease the model complexity , we propose a model assembling method to integrate multiple shape prediction layers into one shape prediction layer .\n",
            " Ren et al .\n",
            " estimated the locations of 5 facial landmarks using Cascaded Convolutional Neural Networks ( Cascaded CNN ) , in which each level computes averaged estimated shape and the shape is refined level by level .\n",
            " Similarly ,\n",
            " The architecture of our network MCL is illustrated in .\n",
            " To sidestep these problems , we operate Global Average Pooling on the last convolutional layer to extract a high - level feature representation x , which computes the average of each feature map .\n",
            " is the set of parameters including weights and biases of our network MCL , which is updated using Mini - Batch Stochastic Gradient Descent ( SGD ) at each iteration .\n",
            " The face alignment loss is defined as\n",
            " It improves the detection precision of each facial landmark by integrating the advantage of each shape prediction layer .\n",
            " \n",
            "Important points in face_alignment - 3 :\n",
            "On the contrary , accurately predicting the values of background pixels ( pixels with zero values ) is less important , since small errors on these pixels will not affect landmark prediction inmost cases .\n",
            " We thus propose a new loss function and name it Adaptive Wing loss ( Sec. , that is able to significantly improve the quality of heatmap regression results .\n",
            " With proposed Weighted Loss Map it is also able to focus on foreground pixels and difficult background pixels during training .\n",
            " We believe including boundary information is beneficial to the heatmap regression and utilized a modified version to our model .\n",
            " If the gradient magnitude is close to zero at this point , then we say the loss function has a small influence at pointy ??.\n",
            " Loss n is the loss of n ?\n",
            " This property could cause training to converge while many pixels still have small errors .\n",
            " L1 loss has constant gradient so that pixels with small errors have the same influence as pixels with large errors .\n",
            " as the threshold , we introduce a new variable ?\n",
            " as a threshold to switch between linear and nonlinear part .\n",
            " where ?\n",
            " \n",
            "Important points in face_alignment - 17 :\n",
            "About \" environment \" effect , distortion brought by explicit image style variance was observed recently .\n",
            " To this end , we propose a new framework to augment training for facial landmark detection without using extra knowledge .\n",
            " Consequently , a face image is decomposed and rendered from distinctive image style and facial geometry .\n",
            " Our work is also related to disentangled representation learning .\n",
            " With this setting , if the latent space of style and shape is mostly uncorrelated , using Cartesian product of z and y latent space should capture all variation included in a face image .\n",
            " Our network consists of a structure estimator E struct to encode landmark heatmaps into structure latent space , a style encoder E style that learns the style embedding of images , and a decoder that re-renders the style and structure to image space .\n",
            " As the prior distribution is commonly assumed to be a unit Gaussian distribution p ? N ( 0 , 1 ) , the learned style feature is regularized to suppress contained structure information through reconstruction .\n",
            " To better reconstruct the original image , E style is enforced to learn structure - invariant style information .\n",
            " Based on the new 68 - point annotation , we conduct more precise evaluation .\n",
            " Cross - dataset evaluation is also provided among existing datasets .\n",
            " However , our model still yields 1.8 % and 3.1 % improvement on LAB and SAN respectively , which manifest the consistent benefit when using the \" style - augmented \" strategy .\n",
            " \n",
            "Important points in face_alignment - 4 :\n",
            "Facial landmarks detection process of Cascaded Regression ( a ) and Self - Iterative Regression ( b ) .\n",
            " ( 3 ) Subsequent regressors usually can not be activated for training until previous regressors finished their training process , which increases the system complexity .\n",
            " The contributions of this paper are summarized as follows :\n",
            " In the testing process , parameter will be sequentially refined by these cascaded regressors in Equ 2 .\n",
            " Random sampling in facial landmarks model\n",
            " Sampling space .\n",
            " For i - th image in the t - th sampling period , sampling parameter S t , i is obtained by random selecting a value in Equ . ( 4 ) .\n",
            " t , i as regressor 's corresponding target increment .\n",
            " For the testing process , similar to the cascaded regression methods , starting from initial location parameters ?\n",
            " 0 , we iteratively update the location parameters ?\n",
            " As discussed before , previous cascaded regression methods adding more regressors can achieve better performance , but increase the number of model parameters , computing resources and storage space , especially for deep learning based methods .\n",
            " \n",
            "Important points in face_alignment - 7 :\n",
            "develops a volumetric representation of 3 D face and uses a network to regress it from a 2D image .\n",
            " All of these are achieved by the elaborate design of the 2D representation of 3 D facial structure and the corresponding loss function .\n",
            " We show that this design helps improving the performance of our network .\n",
            " The first column is the input images from Florence dataset and the Internet , the second column is the reconstructed face from our method , the third column is the results from VRN - Guided .\n",
            " While it 's natural to think that spatially adjacent points could share weights in predicting their positions , which can be easily achieved by using convolutional layers .\n",
            " However , the max number of points is only 1024 , far from enough to represent an accurate 3 D face .\n",
            " Thus our position map can be easily comprehended as replacing the r , g , b value in texture map by x , y , z coordinates .\n",
            " Thus , in order to make full use of this dataset , we conduct the UV coordinates corresponding to BFM .\n",
            " In order to handle images with occlusions , we synthesize occlusions by adding noise texture into raw images , which is similar to the work of .\n",
            " With all above augmentation operations , our training data covers all the difficult cases .\n",
            " The first best result in each category is highlighted in bold , the lower is the better . :\n",
            " \n",
            "Important points in face_alignment - 10 :\n",
            "In addition , different network architectures have been extensively studied during the recent years along with the development of deep neural networks in other AI applications .\n",
            " To further address the issue , we propose a new loss function , namely Wing loss ( ) , for robust facial landmark localisation .\n",
            " Section 2 presents a brief review of the related literature .\n",
            " The second strategy , which has become very popular in recent years , is to use 3 D face models .\n",
            " To obtain this mapping , first , we have to define the architecture of a multi - layer neural network with randomly initialised parameters .\n",
            " , the target of CNN training is to find a ?\n",
            " Wing loss\n",
            " Analysis of different loss functions\n",
            " Finding the minimum in either case is straightforward .\n",
            " However , the situation becomes more complicated when we try to optimise simultaneously the location of multiple points , as in our problem of facial landmark localisation for - mulated in Eq. ( 3 ) .\n",
            " A comparison of different loss functions using our PDB strategy and two - stage landmark localisation framework , measured in terms of the average normalised error ( 10 ?2 ) on AFLW .\n",
            " \n",
            "Important points in face_alignment - 12 :\n",
            "Since this interpretation has gone beyond the sparse set of landmarks , fitting a dense 3 D face model to the face image is a reasonable way to achieve DeFA .\n",
            " All of the in - the - wild face alignment datasets have no more than 68 landmarks in the labeling .\n",
            " With the objective of addressing both challenges , we learn a CNN to fit a 3 D face model to the face image .\n",
            " Other than landmarks , there are other features that are useful to describe the shape of a face , such as contours , pose and face attributes .\n",
            " To compute S fora face , we follow the 3 DMM to represent it by a set of 3D shape bases ,\n",
            " Each basis has Q = 53 , 215 vertices , which are corresponding to vertices overall the other bases .\n",
            " CNN\n",
            " Two branches share the first three convolutional blocks .\n",
            " In the second step , the contour on the estimated 3D shape A can be described as the set of boundary vertices A ( : , i c ) ?\n",
            " R 3 L .\n",
            " IJBA : IARPA Janus Benchmark A ( IJB - A ) is an inthe-wild dataset containing 500 subjects and 25 , 795 images with three landmark , two landmarks at eye centers and one on the nose .\n",
            " \n",
            "Important points in face_alignment - 13 :\n",
            "However , this requirement is expensive to fulfill .\n",
            " Such a small training set is far from adequate to describe the full variability of human faces .\n",
            " Thus , a PCA model is unable to interpret facial variations well .\n",
            " Linear 3 DMM .\n",
            " Since with a single image , present information about the surface is limited ; 3 D face re - construction must rely on prior knowledge like 3 DMM .\n",
            " The recent work of Tewari et al. reconstruct a 3 D face by an elegant encoder - decoder network .\n",
            " R 3 Q of the face is defined within the mean shapeS , which describes the R , G , B colors of Q corresponding vertices .\n",
            " Firstly , a 3 D face is projected onto the image plane with the weak perspective projection model :\n",
            " where ? 1 , ? 2 , ? 1 , ?\n",
            " 2 are constant scale and translation scalars to place the unwrapped face into the image boundaries .\n",
            " Finally , one of key reasons we are able to employ adversarial loss is that we are rendering in the 2D image space , rather than the 3D vertices space or unwrapped texture space .\n",
            " \n",
            "Important points in face_alignment - 0 :\n",
            "That is the reason why the method was only shown to work on real - world , but not \" in - the -wild \" , images .\n",
            " That is , without prior knowledge regarding the shape of the object at - hand there are inherent ambiguities present in the problem .\n",
            " That is the reason why current stateof - the - art 3 D face reconstruction methodologies rely solely on fitting a statistical 3 D facial shape prior on a sparse set of landmarks .\n",
            " However , an orthographic projection model can also be used in the same way .\n",
            " where C is the number of channels of the feature - based image .\n",
            " To alleviate these issues , we cast a ray from the camera to each vertex and test for self - intersections with the triangulation of the mesh in order to learn a per-vertex occlusion mask mi ?\n",
            " Then , P ? ( X ) is defined as the projection of the matrix X on the observed entries ? , namely P ?\n",
            " This results in {t , Ut } , wher ? t ?\n",
            " In order to rapidly adapt the camera parameters in the cost of Eq. 11 , we further expand the where s l = [ x 1 , y 1 , . . . , x L , y L ]\n",
            " T denotes a set of L sparse 2D landmark points ( L N ) defined on the image coordinate system and W l ( p , c ) returns the 2L 1 vector of 2D projected locations of these L sparse landmarks .\n",
            " are the residual terms .\n",
            " \n",
            "Important points in face_alignment - 15 :\n",
            "For each pair of the four results , on the left is the rendering of the fitted 3D shape with the mean texture , which is made transparent to demonstrate the fitting accuracy .\n",
            " In medium poses , this problem can be addressed by changing the semantic positions of face contour landmarks to the silhouette , which is termed landmark marching .\n",
            " Manual labelling landmarks on large - pose faces is a very tedious task .\n",
            " A number of achievements have been made including the classic Active Appearance Model ( AAM ) and Constrained Local Model ( CLM ) .\n",
            " Blanz et al . propose the 3D morphable model ( 3 DMM ) which describes the 3D face space with PCA :\n",
            " exp is the expression parameter .\n",
            " shows the network structure .\n",
            " The output is a 234 - dimensional parameter update including 6 - dimensional pose parameters [ f , pitch , yaw , roll , t 2 dx , t 2 dy ] , 199 - dimensional shape parameters ?\n",
            " The directions of pose parameters always exhibit much higher curvatures than the PCA coefficients .\n",
            " As a result , optimizing VDC with gradient descend converges very slowly due to the \" zig - zagging \" problem .\n",
            " We observe that the fitting error highly depends on the ground truth face posture ( FP ) .\n",
            " \n",
            "Important points in face_alignment - 11 :\n",
            "We exploit this invariance to apply a loss that matches the identity features between the input photograph and a synthetic rendering of the predicted face .\n",
            " Morphable 3D\n",
            " Subsequent work ( and others ) has applied a range of techniques to improve the accuracy and stability of the fitting , producing very accurate results under good conditions .\n",
            " We build on this work and use FaceNet to both produce input features for our regression network , and to verify that the output of the regression resembles the input photograph ..\n",
            " The decoder network consists of two 1024 - unit fully connected + ReLU layers followed by a 398 - unit regression layer .\n",
            " Differentiable Renderer\n",
            " We use the Phong reflection model for shading .\n",
            " Finally , since the Basel Face model does not contain specular color information , we use a heuristic to define specular colors K s from the diffuse colors K d of the predicted model :\n",
            " Loss\n",
            " Applying the identity loss alone allows training to introduce biases into the decoder outputs that change their distribution from the zero- mean standard normal distribution assumption made by the Basel Face Model .\n",
            " The unstructured method of Sela et al. does not sepa-rate identity and expression , predicts only shape , and is less robust than the model - based methods .\n",
            " \n",
            "Important points in face_alignment - 1 :\n",
            "In the following paragraph , we provide examples from 5 predominant approaches :\n",
            " 5 . More recently , the state - of - the - art method of that produces the average ( neutral ) 3 D face from a collection of personal photos , firstly performs landmark detection , then fits a 3 DMM using a sparse set of points , then solves an optimization problem similar to the one in , then performs surface normal estimation as in and finally performs surface reconstruction by solving another energy minimisation problem .\n",
            " We demonstrate that our CNN works with just a single 2 D facial image , does not require accurate alignment nor establishes dense correspondence between images , works for arbitrary facial poses and expressions , and can be used to reconstruct the whole 3 D facial geometry bypassing the construction ( during training ) and fitting ( during testing ) of a 3 DMM .\n",
            " Compared to works based on shape from shading , our method can not capture such fine details .\n",
            " As our target is to apply the method on completely unconstrained images from the web , we chose the dataset of for forming our training and test sets .\n",
            " Note that because each mesh is produced by a 3 DMM , the vertices of all produced meshes are in dense correspondence ; however this is not a prerequisite for our method and unregistered raw facial scans could be also used if available ( e.g. the BU - 4 DFE dataset ) .\n",
            " The importance of spatial alignment is analysed in more detail in Section 5 .\n",
            " Volumetric Regression Networks\n",
            " This stacked representation and architecture is demonstrated in .\n",
            " During training we used the ground truth landmarks while during testing we used a stacked hourglass network trained for facial landmark localisation .\n",
            " Notice that when there is no point correspondence between the ground truth and the estimated mesh , ICP was used but only to establish the correspondence , i.e. the rigid alignment was not used .\n",
            " \n",
            "Important points in face_alignment - 14 :\n",
            "Additionally , face images of interest nowadays usually contain off - angle poses , illumination variations , low resolutions , and partial occlusions .\n",
            " The 3D generic elastic model ( 3D - GEM ) approach was proposed as an efficient and reliable 3D modeling method from a single 2D image .\n",
            " ( 2 ) Our approach is efficiently implemented in an endto - end deep learning framework allowing for the alignment and 3D modeling tasks to be codependent .\n",
            " CNNs for 3D Object Modeling\n",
            " We approach finding the unknown camera projection matrix parameters and the parameters needed to generate the 3D model of the head in a similar fashion .\n",
            " Additionally , the pose of the face can be determined from the camera parameters allowing fora visibility map to be generated for the 3D model .\n",
            " In order to perform backpropogation on the new grid generator , the derivative of the generated grid with respect to a must be computed .\n",
            " The gradient with respect to each of the rows of M can be shown to be\n",
            " In order to make use of the TPS warped 3D points in the camera projection module of the transformer network , the module must take in as input the warped coordinates .\n",
            " This means that such a module would also have to do backpropogation on the 3D coordinates as well as the camera projection parameters .\n",
            " 300 W - LP :\n",
            " \n",
            "Important points in coreference_resolution - 6 :\n",
            "Regarding the entire input corpus as one sequence instead of a batch also significantly increases the time complexity of the model .\n",
            " A popular method of co-reference resolution is mention ranking .\n",
            " and proposed models using global entity - level features .\n",
            " We present the input embeddings of the j - th word in the i - th sentence with x i , j .\n",
            " The model is optimized with the Adam algorithm ( Kingma and Ba , 2014 ) .\n",
            " Experiment Results and Discussion\n",
            " - The ship was therefor refueling .\n",
            " This gives the model a better ability to model cross - sentence dependency .\n",
            " \n",
            "Important points in coreference_resolution - 3 :\n",
            "Our experiments show that both of the above contributions improve the performance of coreference resolution on the English OntoNotes benchmark .\n",
            " and all preceding spans .\n",
            " where s ( i , j ) is a pairwise score for a coreference link between span i and span j.\n",
            " The learned gate vector f n i determines for each dimension whether to keep the current span information or to integrate new information from its expected antecedent .\n",
            " Keep the top K antecedents of each remaining span i based on the first three factors , s m ( i ) + s m ( j ) + s c ( i , j ) .\n",
            " The soft higher - order inference from Section 3 is computed in this final stage .\n",
            " 2 . using 3 highway LSTMs instead of 1 .\n",
            " On the development set , the second - order model ( N = 2 ) outperforms the first - order model by 0.8 F1 , but the third order model only provides an additional 0.1 F1 improvement .\n",
            " \n",
            "Important points in coreference_resolution - 4 :\n",
            "[ AnaphS\n",
            " These representations are combined into a joint representation used to calculate a score that characterizes the relation between them .\n",
            " We evaluate our model on the shell noun resolution dataset of and show that it outperforms their state - of - the - art results .\n",
            " ( 2 ) Congress has focused almost solely on the fact that [ special education is expensive - and that it takes away money from regular education . ]\n",
            " The resulting vector is fed into a feed - forward layer of exponential linear units ( ELUs ) to produce the final representationh s orh c of the sequence .\n",
            " Finally , we compute the score for the pair ( c , s ) that represents relatedness between them , by applying a single fully connected linear layer to the joint representation :\n",
            " In ( 4 ) , the verb doubt establishes a specific semantic relation between the embedding sentence and its sentential complement .\n",
            " The remaining rows in apply to adverbial clauses of different types .\n",
            " 13 ARRAU distinguishes abstract anaphors and ( mostly ) pronominal anaphors referring to an action or plan , as plan .\n",
            " Candidates extraction .\n",
            " Regularization .\n",
            " \n",
            "Important points in coreference_resolution - 7 :\n",
            "While previous approaches employed the ELMo model , we propose to use BERT embeddings , motivated by the impressive empirical performance of BERT on other tasks .\n",
            " The first use of BERT embeddings in coreferenceresolution .\n",
            " Baseline Model\n",
            " To illustrate the difference between Entity Equalization and antecedent averaging , consider the following example : \" [ John ] went to the park and [ he ] got tired .\n",
            " In order to obtain BERT embeddings for sequences of unlimited length , we propose to use BERT in a convolutional mode as follows .\n",
            " We then take the four last layers of the BERT model for token i and apply a learnable weighted averaging to those , similar to the process used in ELMo .\n",
            " Mention - entity mappings have been used in the context of optimizing coreference performance measures .\n",
            " Our experimental setup is very similar to , and our code is built on theirs .\n",
            " \n",
            "Important points in coreference_resolution - 9 :\n",
            "Moreover , the overlap variant , which artificially extends the context window beyond 512 tokens provides no improvement .\n",
            " Overview of c2f- coref\n",
            " Applying BERT\n",
            " We extend the original Tensorflow implementations of c 2f - coref 3 and BERT .\n",
            " While their feature - based approach is more memory efficient , the fine - tuned model seems to yield better results .\n",
            " We performed a qualitative comparison of ELMo and BERT models ) on the OntoNotes English development set by manually assigning error categories ( e.g. , pronouns , mention paraphrasing ) to incorrect predicted clusters .\n",
            " We focus on three observations -( a ) shows how models perform distinctly worse on longer documents , ( b ) both models are unable to use larger segments more effectively ) and perform worse when the max segment len of 450 and 512 are used , and , ( c ) using overlapping segments to provide additional context does not improve results .\n",
            " Modeling pronouns especially in the context of conversations , continues to be difficult for all models , perhaps partly because c 2 f - coref does very little to model dialog structure of the document .\n",
            " \n",
            "Important points in coreference_resolution - 5 :\n",
            "Coreference resolution is fundamentally a clustering task .\n",
            " Z under some scoring function , with Z the set of valid clusterings .\n",
            " First , inference is relatively simple and efficient , requiring only a left - to - right pass through a document 's mentions during which a mention 's antecedents ( as well as ) are scored and the highest scoring antecedent is predicted .\n",
            " found that incorporating cluster - level features beyond those involving the precomputed mention - pair and mention - ranking probabilities that form the basis of their agglomerative clustering coreference system did not improve performance .\n",
            " First , following define ? a ( x n ) : X ?\n",
            " We then use a non-linear feature embedding h c to map a mention x n to a vector - space representation .\n",
            " We begin by defining the local model f ( x n , y) with the two layer neural network of , which has a specialization for the nonanaphoric case , as follows :\n",
            " [ you ] , h . There are currently four entity clusters in scope X ( 1 ) , X ( 2 ) , X ( 3 ) , X ( 4 ) based on unseen previous decisions ( y ) .\n",
            " On the other hand , training on oracle clusters leads to a mismatch between training and test , which can hurt performance .\n",
            " Search\n",
            " In experiments , we set ha ( x n ) , h c ( x n ) , and h ( m ) to be ? R 200 , and hp ( x n , y) ? R 700 .\n",
            " \n",
            "Important points in coreference_resolution - 2 :\n",
            "We evaluate the models on the English and Chinese portions of the CoNLL 2012 Shared Task .\n",
            " The candidate antecedent maybe any mention that occurs before min the document or NA , indicating that m has no antecedent .\n",
            " These features are concatenated to produce an Idimensional vector h 0 , the input to the neural network .\n",
            " We fix ?\n",
            " Otherwise the model is trained in the same way as with the heuristic loss .\n",
            " We can define a probability distribution over actions using the mention - ranking model 's scoring function as follows :\n",
            " Our experiments were run using predicted mentions from Stanford 's rule - based coreference system .\n",
            " We find that REINFORCE does slightly better than the heuristic loss , but reward rescaling performs significantly better than both on both languages .\n",
            " Mention - ranking models have been widely used for coreference resolution .\n",
            " These models are typically trained with heuristic loss functions that assign costs to different error types , as in the heuristic loss we describe in Section 3.1 .\n",
            " \n",
            "Important points in coreference_resolution - 8 :\n",
            "The model factors , for example , directly indicate whether an absent coreference link is due to low mention scores ( for either span ) or a low score from the mention ranking component .\n",
            " However , all of these models use parsers for head features and include highly engineered mention proposal algorithms .\n",
            " Task\n",
            " A challenging aspect of this model is that it s size is O ( T 4 ) in the document length .\n",
            " To further reduce the number of spans to consider , we only keep up to ?\n",
            " We accept spans in decreasing order of the mention scores , unless , when considering span i , there exists a previously accepted span j such that\n",
            " Fixing score of the dummy antecedent to zero removes a spurious degree of freedom in the over all model with respect to mention detection .\n",
            " We experimented with these cost-sensitive alternatives ,\n",
            " We also report ensemble experiments using five models trained with different random initializations .\n",
            " Ensembling is performed for both the span pruning and antecedent decisions .\n",
            " With oracle mentions , we see an improvement of 17.5 F1 , suggesting an enormous room for improvement if our model can produce better mention scores and anaphoricity decisions .\n",
            " \n",
            "Important points in coreference_resolution - 0 :\n",
            "This approach allows the model to learn which action ( a cluster merge ) available from the current state ( a partially completed coreference clustering ) will eventually lead to a high - scoring coreference partition .\n",
            " System Architecture\n",
            " The system architecture is summarized in .\n",
            " First , it serves as pretraining for the cluster - ranking model ; in particular the mentionranking model learns effective weights for the mention - pair encoder .\n",
            " We address this problem by training a cluster - ranking model that scores pairs of clusters instead of pairs of mentions .\n",
            " where W c is a 1 2 d weight matrix .\n",
            " Head match , exact string match , and partial string match .\n",
            " However , we found these features were crucial forgetting good model performance .\n",
            " We also find it beneficial to prune the set of candidate antecedents A ( m ) for each mention m .\n",
            " Rather than using all previously occurring mentions as candidate antecedents , we only include high - scoring ones , which greatly reduces the size of the search space .\n",
            " 4 ) . We generally report the average F 1 score ( CoNLL F 1 ) of the three , which is common practice in coreference evaluation .\n",
            " \n",
            "Important points in coreference_resolution - 1 :\n",
            "In this paper , we propose a goal - directed endto - end deep reinforcement learning framework to resolve coreference as shown in .\n",
            " Related Work\n",
            " Furthermore , we add entropy regularization to encourage more exploration ) and prevent our model from prematurely converging to a sub-optimal ( or bad ) local optimum .\n",
            " Strube , 2010 ) as the reward .\n",
            " where ? expr ?\n",
            " expr is , the more diverse our model can explore .\n",
            " Besides , we set the number of sampled trajectories N s = 100 , tune the regularization parameter ? expr in { 10 ?5 , 10 ?4 , 0.001 , 0.01 , 0.1 , 1 } and set it to 10 ? 4 based on the development set .\n",
            " In , we compare our model with the coreference systems that have produced significant improvement over the last 3 years on the OntoNotes benchmark .\n",
            " We present the first end - to - end reinforcement learning based coreference resolution model .\n",
            " Our model transforms the supervised higher order coreference model to a policy gradient model that can directly optimizes coreference evaluation metrics .\n",
            " \n",
            "Important points in face_detection - 8 :\n",
            "According to the size of the general face , we carefully design size of anchors and RoIs .\n",
            " ( 2 ) We introduce a novel position - sensitive average pooling to re-weight embedding responses on score maps and eliminate the effect of non-uniformed contribution in each facial part .\n",
            " The DPM methods detect faces by modeling the relationship of deformable facial parts .\n",
            " Based Architecture\n",
            " Formally , let X = { X i | i = 1 , 2 , ... , M } denote the output M feature maps of a position - sensitive RoI pooling layer , and X i = {x i , j |j = 1 , 2 , ... , N 2 } denote the i th feature map , where N denotes the size of the pooled feature map .\n",
            " Note that position - sensitive average pooling can bethought as performing feature embedding on every location of responses followed by average pooling .\n",
            " Example images of WIDER FACE and FDDB are shown in .\n",
            " In terms of the RPN stage , Face R - FCN enumerates multiple configurations of the anchor in order to accurately search for faces .\n",
            " Furthermore ,\n",
            " We expand the training dataset by augmenting with a privately collected dataset and use the enlarged dataset to train a more discriminative face detector ( denoted as Model - B ) .\n",
            " \n",
            "Important points in face_detection - 9 :\n",
            "We make use of a coarse image pyramid to capture extreme scale challenges in ( c ) .\n",
            " Much recent work in object detection makes use of scale - normalized classifiers ( e.g. , scanning - window detectors run on an image pyramid or region - classifiers run on \" ROI \" - pooled image features ) .\n",
            " We provide two remaining key insights to the problem of finding small objects .\n",
            " This suggests that context should be modeled in a scale - variant manner .\n",
            " Exploring context and resolution\n",
            " By explicitly factoring out scale - variation in terms of the desired output , we can explore the role of context and the canonical template size .\n",
            " Figure 4 :\n",
            " Same colors are used in presents an analysis of the effect of context , as given by the size of the receptive field ( RF ) used to make heatmap prediction .\n",
            " We verify this hypothesis in the next section , where we describe a pipeline for building scale - specific detectors with varying canonical resolutions .\n",
            " Approach : scale-specific detection\n",
            " Natural regimes emerge in the figure : for finding large faces ( more than 140 px in height ) , build templates at 0.5 resolution ; for finding smaller faces ( less than 40 px in height ) , build templates at 2 X resolution .\n",
            " \n",
            "Important points in face_detection - 7 :\n",
            "Models in such a philosophy probably have the highest recall as long as the sampling of scales is dense enough , but they suffer from high computation cost and more false positives .\n",
            " In this paper , we extend the spirit of fast feature pyramid to CNN and go a few steps further .\n",
            " Such a scheme could feed the network with input at one scale only and approximate the rest features at smaller scales through a learnable RSA unit - a balance considering both efficiency and accuracy .\n",
            " Liu et al .\n",
            " 2 ? , respectively .\n",
            " Given observations x , the distribution , parameterized by ? , can be decomposed into K mixture components :\n",
            " Given an input image I , I m denotes the downsampled result of the image with a ratio of 1 / 2 m , where m ?\n",
            " where f ( ) stands for a set of convolutions with a total stride of 8 from the input image to the output map .\n",
            " To this end , we have the revised loss for our landmark retracing network :\n",
            " ( 11 ) Apart from the detection - to - landmark design as previous work did , our retracing network also fully leverages the feature set of landmarks to help rectify the confidence of identifying a face .\n",
            " The ratio of the positive and the negative is 1 : 1 in all experiments .\n",
            " \n",
            "Important points in face_detection - 17 :\n",
            "SSH and S 3 FD develop scale - invariant networks to detect faces with different scales from different layers in a single network .\n",
            " To achieve this goal , extra labels are needed and the anchors matched to these parts should be designed .\n",
            " In addition , we propose a training strategy named as Data - anchor - sampling to make an adjustment on the distribution of the training dataset .\n",
            " Recently , FPN - style framework achieves great performance on both objection detection and face detection .\n",
            " Recently , FPN - style framework achieves great performance on both objection detection and face detection .\n",
            " So it is rude to directly use all high - level features to enhance the performance on small faces .\n",
            " Notice that the outputs of CPM are used for supervising pyramid anchors , see Sec. 3.3 , which approximately cover face , head and body region in our experiments .\n",
            " We design a PyramidBox Loss .\n",
            " Besides , a Pyramid Box Loss will be demonstrated in Sec. 3.4 .\n",
            " In our experiments , we set the hyper parameter spa = 2 since the stride of adjacent prediction modules is 2 .\n",
            " The term p * k , i L k, reg means the regression loss is activated only for positive anchors and disabled otherwise .\n",
            " \n",
            "Important points in face_detection - 5 :\n",
            "However , due to its complex CNN structure , this approach is time costly in practice .\n",
            " However , most of the available face detection and face alignment methods ignore the inherent correlation between these two tasks .\n",
            " However , traditional hard sample mining usually performs an offline manner , which significantly increases the manual operations .\n",
            " However , we noticed its performance might be limited by the following facts :\n",
            " For example , for the sample of background region , we only compute , and the other two losses are set as 0 .\n",
            " where is the number of training samples .\n",
            " III .\n",
            " WIDER FACE dataset consists of 393,703 labeled face bounding boxes in 32,203 images where 50 % of them for testing into three subsets according to the difficulty of images , 40 % for training and the remaining for validation .\n",
            " ( e ) shows that our method outperforms all the state - of - the - art methods with a margin .\n",
            " F. Runtime efficiency\n",
            " \n",
            "Important points in face_detection - 20 :\n",
            "Channel features compute registered maps of the original images like gradients and histograms of oriented gradients and then extract features on these extended channels .\n",
            " Through the deep exploration , we find that : 1 ) multi-scaling the feature representation further enriches the representation capacity since original aggregate channel features have uniform feature scale ; 2 ) different combinations of channel types impact the performance greatly , while for face detection the color channel in LUV space , plus gradient magnitude channel and gradient histograms channels in RGB space show best result ; 3 ) multi-view detection is proven to be a good match with aggregate channel features as the representation naturally encodes the facial structure ( ) .\n",
            " Section 4 addresses problems concerning multi-view face detection .\n",
            " Each design part is divided into several separate experiments ended with a summary explaining the specific parameters used in our proposed face detector .\n",
            " 1 ) The image channels are extended to more types in order to encode diverse information like color , gradients , local histograms and soon , therefore possess richer representation capacity .\n",
            " 3 ) Due to its structure consistence with the overall image , when coupled with boosting method , the boosted classifier naturally encodes structured pattern information from large training data ( see for an illustration ) , which gives more accurate localization of faces in the image .\n",
            " Three types of channels are used , which are color channel ( Gray - scale , RGB , HSV and LUV ) , gradient magnitude , and gradient histograms .\n",
            " It can be seen that the gradient histograms contribute most to the performance among all three channel types .\n",
            " As for multi-scale version of aggregate channel features , multi-local - scale with an additional scale of radius 2 shows the best performance .\n",
            " The probable reason is that pre-smoothing controls the local scale of the neighborhood feature correlations and therefore matches the intuition inside multi-scale best .\n",
            " Second , as for detection evaluation , usually the overlap of bounding boxes is used as the criterion .\n",
            " \n",
            "Important points in face_detection - 3 :\n",
            "Face images annotated ( red ellipses ) in the FDDB database. , have been proposed to replace the Haarlike features used in .\n",
            " In fact , since these methods require partitioning multiview data into known poses , occlusion is not easy to handle in this way .\n",
            " Secondly , we propose a deep quadratic tree learning method and construct a single soft - cascade AdaBoost classifier to handle complex face manifolds and arbitrary pose and occlusion conditions .\n",
            " Lienhart and Maydt proposed an extended set of Haar - like features , where 45 rotated rectangular features were introduced .\n",
            " In this paper , we show that the optimal ordinal / contrastive features and their combinations can be learned by integrating the proposed NPD features in a deep quadratic tree .\n",
            " Jones and Viola extended their face detector by training one face detector for each specific pose .\n",
            " This is a simpler approach to multiview face detection than designing complex cascade structures .\n",
            " They hence proposed a multiclassifier boosting ( MCBoost ) for human perceptual clustering of object images , which showed promise for clustering face poses .\n",
            " We use subscripts to differentiate between pixel and pixel values only when pixel locations are under discussion .\n",
            " The NPD feature measures the relative difference between two pixel values .\n",
            " Therefore , in this paper , we consider a quadratic splitting strategy and a deeper tree structure .\n",
            " \n",
            "Important points in face_detection - 2 :\n",
            "Analyzing with anchor-based face detectors , they detect faces by classifying and regressing a series of preset anchors , which are generated by regularly tiling a collection of boxes with different scales and aspect ratios .\n",
            " Det addresses this issue using a preset threshold to filter out negative anchors .\n",
            " As shown in ( d ) , as the IoU threshold increases , the AP drops dramatically , indicating that the accuracy of the bounding box location needs to be improved .\n",
            " Faceness formulates face detection as scoring facial parts responses to detect faces under severe occlusion .\n",
            " At each pyramid level , we use two specific scales of anchors ( i.e. , 2S and 2 ?\n",
            " 362 pixels across levels with respect to the network 's input image .\n",
            " Therefore , the classification task is relatively easy in these three higher pyramid levels .\n",
            " As the statistical result shown in , the STC increases the positive / negative sample ratio by approximately 38 times , from around 1:15441 to 1:404 .\n",
            " At present , most detection networks utilize ResNet and VG - GNet as the basic feature extraction module , while both of them possess square receptive fields .\n",
            " The singleness of the receptive field affects the detection of objects with different aspect ratios .\n",
            " Finally , we apply the non-maximum suppression ( NMS ) with jaccard overlap of 0.5 to generate the top 750 high confident detections per image as the final results .\n",
            " \n",
            "Important points in face_detection - 21 :\n",
            "These detectors are flexible and robust to both pose variation and partial occlusion , since they can reliably detect the faces based on some confident part detections .\n",
            " To address the conflicting challenge , Li et al. proposed a cascade DNN architecture at multiple resolutions .\n",
            " The aligned candidate face region is then fed into the second - stage network , a RCNN , for further verification .\n",
            " The first stage is a multi -task Region Proposal Network ( RPN ) .\n",
            " 2 ) The learning of the prediction model for the facial landmark is supervised by the ground - truth facial landmark points .\n",
            " Specifically , we use the following formula to define a similarity transformation , i.e. ,\n",
            " The I x and I y are horizontal and vertical gradient of the original image\n",
            " And the gradient with respect to the detected facial landmarks :\n",
            " That is the reason why current DNN - based models heavily rely on a high - end GPU to increase the runtime performance .\n",
            " However , high - end GPU is not often available in commodity computing system , so most often , we still need to run the DNN model with a CPU .\n",
            " Since the smallest face size can be detected by the proposed DNN based face detector is 36 36 pixels , the first group contains the face size between 36 to 72 pixels .\n",
            " \n",
            "Important points in face_detection - 19 :\n",
            "Our key contributions are summarized as follows : ( 1 ) A light head based two - stage framework named FDNet1.0 is developed for face detection .\n",
            " Based on this groundbreaking work , more advanced hand - engineered features and more powerful machine learning algorithms are developed to improve face detection performance .\n",
            " S 3 FD presents a single shot scale - invariant face detector which achieves good result on WIDER FACE datasets .\n",
            " Additionally , anchors are carefully designed to obtain better location samples .\n",
            " The anchors with highest IoU score or IoU score with the ground truth above 0.7 are defined as positive .\n",
            " The anchors with IoU score with the ground truth above 0.5 are assigned as positive , IoU score that is lower than 0.3 is defined as negative , IoU score above 0.3 but lower than 0.5 will be ignored .\n",
            " Conclusion\n",
            " We will also consider some ideas for faster inference speed , e.g. designing light backbone .\n",
            " \n",
            "Important points in face_detection - 4 :\n",
            "Besides , face detectors are always deployed on edge devices , such as mobile phones , IP cameras and IoT ( Internet of Things ) sensors .\n",
            " In our view , to better balance accuracy and latency is crucial for applying face detection to more applicable areas .\n",
            " Whereas , one - stage methods coherently combine classification and bounding box ( bbox ) regression , always achieving anchor- based and multi-scale detection simultaneously .\n",
            " We introduce the RF to overcome the drawbacks of the previous anchor - based strategies , resulting in a anchorfree method .\n",
            " FaceBoxes aims to make the face detector run in real - time by rapidly reducing the size of input images .\n",
            " Different from FaceBoxes , our method handles the detection of small faces delicately , achieving fast running speed and large scale coverage in the meantime .\n",
            " Intuitively , the target object can be well detected with high probabilities if it is enclosed by a certain RF .\n",
            " This phenomenon is named as effective receptive field ( ERF ) .\n",
            " In spite of improving the anchor hit rate , S3 FD induces the anchor imbalance problem ( too many anchors for tiny faces ) which has to be addressed by additional means .\n",
            " However , the proposed method can achieve 100 % face coverage in theory by controlling the RF stride .\n",
            " Since big faces are much easier to detect , the ratios of RFs and average face scales are relatively small .\n",
            " \n",
            "Important points in face_detection - 18 :\n",
            "In other words , this process tries to mimic the way of face detection by human in a sense that when humans are not sure about a face , seeing the body will increase our confidence .\n",
            " Our approach is evaluated on two challenging face detection databases and compared against numerous recent face detection methods .\n",
            " Section 3 reviews a general deep learning framework , the background as well as the limitations of the Faster R - CNN in the problem of face detection .\n",
            " Neural Networks have been around fora longtime but have been experiencing a resurgence in popularity due to hardware improvements and new techniques resulting in the capability to train these networks on large amounts of training data .\n",
            " Due to the lack of labeled training data , it adopts a strategy of supervised pre-training for an auxiliary task followed by domain - specific fine - tuning .\n",
            " The method achieves high accuracy but is very time - consuming .\n",
            " An RPN is implemented in a fully convolutional style to predict the object bounding boxes and the objectness scores .\n",
            " Limitations of Faster R - CNN\n",
            " The reason is that in these networks both the proposed region and the classification score are produced from one single high - level convolution feature map .\n",
            " This representation does n't have enough information for the multiple tasks , i.e. region proposal and RoI detection .\n",
            " Sometimes a human body makes us more convinced about the existence of a face .\n",
            " \n",
            "Important points in face_detection - 6 :\n",
            "As shown in left part of , over two thirds of the training images already obtained perfect detection accuracy , which indicates that those easy images are less useful towards training a robust face detector .\n",
            " We show this strategy can make our detector more robust towards hard faces , without involving more complex network architecture and computation overhead .\n",
            " We propose a hard image mining strategy , to improve the robustness of our detector to those extremely hard faces .\n",
            " Recent state - of - the - art face detectors are generally built based on Faster - RCNN , R - FCN or SSD .\n",
            " Similar to SSH , we apply 11 convolution layers after conv4 3 and conv5 3 to reduce dimension , and then apply a 33 convolution layer on the concatenation of these two dimension reduced features .\n",
            " We attach three anchors at each point in the grid as default face detection boxes .\n",
            " The intuition is that in order to detect faces with different sizes , different effective receptive fields are required .\n",
            " With this design , the input of the 3 3 convolution , will be aligned to the same location of faces , regardless of the size of faces and anchors .\n",
            " Anchor - level hard example mining\n",
            " OHEM has been proven to be useful for object detection and face detection in .\n",
            " We show the discontinuous ROC curve at compared with , and our method achieves the state - of - the - art performance of TPR = 98.7 % given 1000 false positives .\n",
            " \n",
            "Important points in face_detection - 15 :\n",
            "In order to meet these two conflicting requirements , face detection has been intensely studied mainly in two ways .\n",
            " Recently , CNN has been successfully introduced into the face detection task as feature extractor in the traditional face detection framewrok .\n",
            " However , there are three problems in cascaded CNN based methods :\n",
            " Besides the cascade framework , methods based on structural models progressively achieve better performance and become more and more efficient .\n",
            " Finally , we introduce the associated training methodology .\n",
            " More precisely , the convolution operation for CPU is extremely time - consuming when the size of input , kernel and output are large .\n",
            " From this observation , C. ReLU can double the number of output channels by simply concatenating negated outputs before applying ReLU .\n",
            " However , as a stand - alone face detector , RPN is notable to obtain competitive performances .\n",
            " Our model is trained on 12 , 880 images of the WIDER FACE training subset .\n",
            " Data augmentation .\n",
            " Comparing with AFW and PASCAL face , FDDB is much more difficult so that analyzing our model on FDDB is convincing .\n",
            " \n",
            "Important points in face_detection - 11 :\n",
            "In natural images , objects can appear at very different scales , as illustrated by the yellow bounding boxes .\n",
            " Similar to , this network consists of two sub-networks : an object proposal network and an accurate detection network .\n",
            " This is achieved by introducing a deconvolutional layer that increases the resolution of feature maps ( see ) , enabling small objects to produce larger regions of strong response .\n",
            " GoogLe Net proposed the use of three weighted classification losses , applied at layers of intermediate heights , showing that this type of regularization is useful for very deep models .\n",
            " The multi-scale mechanism of the RPN , shown in , is similar to that of .\n",
            " As shown in , the single scale of the feature maps , dictated by the ( 228228 ) receptive field of the CNN , can be severely mismatched to small ( e.g. 3232 ) or large ( e.g. 640640 ) objects .\n",
            " The buffer convolution prevents the gradients of the detection branch from being back - propagated directly to the trunk layers .\n",
            " .\n",
            " The learning rate is set to 0.0005 , and reduced by a factor of 10 times after every 10,000 iterations .\n",
            " Learning stops after 25,000 iterations .\n",
            " Context has been shown useful for object detection and segmentation .\n",
            " \n",
            "Important points in face_detection - 14 :\n",
            "We show example images ( cropped ) and annotations .\n",
            " As we will demonstrate , the limitations of datasets have partially contributed to the failure of some algorithms in coping with heavy occlusion , small scale , and atypical pose .\n",
            " We benchmark four representative algorithms , either obtained directly from the original authors or reimplemented using open - source codes .\n",
            " AFW , FDDB , and PASCAL FACE\n",
            " If a face is occluded , we still label it with a bounding box but with an estimation on the scale of occlusion .\n",
            " Each annotation is labeled by one annotator and cross - checked by two different people . The proposals are generated by using Edgebox .\n",
            " The results suggest that WIDER FACE is a more challenging face detection benchmark compared to exist -.\n",
            " X-axis denotes for event class name .\n",
            " The second stage gives face and non-face prediction of the candidate windows generate from first stage .\n",
            " If the candidate window is classified as face , we further refine the location of the candidate window .\n",
            " The best performance is achieved by Faceness , with a recall below 20 % .\n",
            " \n",
            "Important points in face_detection - 13 :\n",
            "Different from generic object detection , face detection features smaller ratio variations ( from 1:1 to 1:1.5 ) but much larger scale variations ( from several pixels to thousand pixels ) .\n",
            " Due to training data limitation , JDA , MTCNN and STN have not verified whether tiny face detection can benefit from the extra supervision of five facial landmarks .\n",
            " Recently , self - supervised 3D morphable models have achieved promising 3 D face modelling in - the - wild .\n",
            " The champion solution of the WIDER Face Challenge 2018 indicates that rigid ( expansion ) and non-rigid ( deformation ) context modelling are complementary and orthogonal to improve the performance of face detection .\n",
            " Similarly , graph convolution also employs the same concept as shown in .\n",
            " R n6 is a set of face vertices containing the joint shape and texture information , and E ?\n",
            " camera location , camera pose and focal length ) and illumination parameters\n",
            " Dataset\n",
            " More specifically , we sort negative anchors by the loss values and select the top ones so that the ratio between the negative and positive samples is at least 3:1 .\n",
            " Data Augmentation .\n",
            " RetinaFace significantly decreases the normalised mean errors ( NME ) from 2.72 % to 2.21 % when compared to MTCNN .\n",
            " \n",
            "Important points in face_detection - 16 :\n",
            "We borrow this term from which uses it to denote a stack of local histograms for multilevel image coding .\n",
            " In order to learn the tasks , we train them simultaneously using multiple loss functions .\n",
            " This paper makes the following contributions .\n",
            " Zhang et al.\n",
            " Although much work has been done for localizing landmarks for frontal faces , limited attention has been given to profile faces which occur more often in real world scenarios .\n",
            " Zhu et al.\n",
            " Separate features were computed for different attributes and they were used to train individual SVMs for each attribute .\n",
            " In this section , we provide a brief overview of the system and then discuss the different components in detail .\n",
            " It contains 25 , 993 faces in 21 , 997 real - world images with full pose , expression , ethnicity , age and gender variations .\n",
            " It provides annotations for 21 landmark points per face , along with the face bounding - box , face pose ( yaw , pitch and roll ) and gender information .\n",
            " Here , ( p 0 , p 1 ) is the two dimensional probability vector computed from the network .\n",
            " \n",
            "Important points in face_detection - 10 :\n",
            "To further address the class imbalance problem , Lin et al.\n",
            " In this paper , we propose three novel techniques to address the above three issues , respectively .\n",
            " In summary , the main contributions of this paper include :\n",
            " Girshick et al. propose smooth L 1 loss to prevent exploding gradients .\n",
            " Module is able to enhance original features to make them more discriminable and robust , which is called FEM for short .\n",
            " where c i , j,l is a cell located in ( i , j ) coordinate of the feature maps in the l - th layer , f denotes a set of basic dilation convolution , elem - wise production , up - sampling or concatenation operations .\n",
            " is a weight to balance the effects of the two terms .\n",
            " Note that anchor size in the first shot is half of ones in the second shot , and ?\n",
            " Feature Enhance Module First ,\n",
            " We adopt anchor designed in S3FD , PyramidBox and six original feature maps generated by VGG16 to perform classification and regression , which is named Face SSD ( FSSD ) as the baseline .\n",
            " shows more examples to demonstrate the effects of DSFD on handling faces with various variations , in which the blue bounding boxes indicate the detector confidence is above 0.8 .\n",
            " \n",
            "Important points in face_detection - 12 :\n",
            "WSMA - Seg has the following advantages : ( i ) as an NMS - free solution , WSMA - Seg avoids all hyperparameters related to anchor boxes and NMS ; so , the above - mentioned threshold selection problem is also avoided ; ( ii ) the complex occlusion problem can be alleviated by utilizing the topological structure of segmentation - like multimodal annotations ; and ( iii ) multimodal annotations are pixel - level annotations ; so , they can describe the objects more accurately and overcome the above - mentioned environment noise problem .\n",
            " We have conducted extensive experimental studies on the Rebar Head , WIDER Face , and MS COCO datasets ; the results show that the proposed WSMA - Seg approach outperforms the state - of - the - art detectors on all testing datasets .\n",
            " Pixel - level segmentation annotations are much more representative than bounding box annotations , so they can resolve some extreme cases that are challenging for bounding box annotations .\n",
            " As shown in , given a test image as the input of the segmentation model , WSMA - Seg first generates three heatmaps , i.e. , interior , boundary , and boundary on interior heatmaps , which are denoted as I , B , and O , respectively .\n",
            " Rebar Head Detection\n",
            " Performing object detection on this dataset is very challenging , because it only contains a few training samples and also encounters very severe occlusion situations ( see ) .\n",
            " WIDER Face has defined three levels of difficulties ( i.e. , Easy , Medium , and Hard ) , based on the detection accuracies of EdgeBox .\n",
            " The experimental results in terms of F1 score are shown in .\n",
            " \n",
            "Important points in face_detection - 0 :\n",
            "In this work , we first modify the popular one - stage RetinaNet method to perform face detection as our baseline model .\n",
            " The pioneering work of Viola and Jones uses the Haarlike feature and the AdaBoost strategy to train several cascaded face detectors , achieving a very good tradeoff between accuracy and efficiency in some simple and fixed scenarios .\n",
            " Faceness obtains different scores according to the spatial structure and arrangement of facial parts to detect faces under severe occlusion and unconstrained pose variations .\n",
            " Starting from the RetinaNet face detector baseline , we apply some recently proposed strategies to achieve state - of - the - art performance on the challenging WIDER FACE dataset .\n",
            " To solve the aforementioned two issues , Selective Two - step Classification ( STC ) and Selective Two - step Regression ( STR ) are proposed in SRN and we follow these designs to further improve the performance of our face detector .\n",
            " STR performs is two - step regression on three high level detection layers to adjust anchors and provide better initialization for the subsequent regressor .\n",
            " Max - out Label\n",
            " Multi-scale Testing\n",
            " Evaluation Result\n",
            " Conclusion\n",
            " \n",
            "Important points in face_detection - 1 :\n",
            "Since they use a large classification network such as VGG - 16 , ResNet - 50 or 101 , and DenseNet - 169 , the number of total parameters exceed 20 million , over 80 Mb supposing 32 - bit floating point for each parameter .\n",
            " As in the figure , we design a backbone network such that reduces the size of the feature map by half , and we can get the other feature maps with recurrently passing the network .\n",
            " We evaluated the proposed detector and its variants on WIDER FACE dataset , the most widely used and similar to the in - the - wild situation .\n",
            " The method achieves comparable accuracy to the original models , and the overall model size is extremely smaller as well .\n",
            " The classification and regression heads are designed by a 3x3 convolutional network and hence , both models are designed as a fully convolutional network .\n",
            " For all the cases , we set the image x to have 640x640 resolution in training phase and use N = 6 number of feature maps .\n",
            " The default setting of the network depth is set to 6 or 8 , and the output channel width is set to 32,48 or 64 , which do not largely exceed overall 0.1 million parameters .\n",
            " The figures in ( a ) and ( b ) each shows the inverted residual block architecture .\n",
            " Experiments\n",
            " In this section , we quantitatively and qualitatively analyze the proposed method with various ablations .\n",
            " This is also meaningful result in that our method did not use any kind of pre-training of the backbone network using the other dataset such as Image Net.\n",
            " \n",
            "Important points in entity_linking - 8 :\n",
            "Through these experiments , we show that :\n",
            " RELIC learns better representations of entity properties if it is trained to match just the contexts in which entities are mentioned , and not the surface form of the mention itself .\n",
            " The Wikification task , in particular , is similar to the work presented in this paper , as it requires systems to map mentions to the Wikipedia pages describing the entities mentioned .\n",
            " in particular take an approach that is very similar in motivation to RELIC , but which focuses on learning entity representations for use as features in downstream classifiers that model non-linear interactions between a small number of candidate entities .\n",
            " and we train RELIC by maximizing the average log probability 1 | D |\n",
            " We sample K negative entities from a noise distribution p noise ( e ) : e 1 , e 2 , . . . , e K ? p noise ( e ) ( 4 ) Denoting e 0 := e , we then compute our per-example loss using cross entropy :\n",
            " We train the model using Tensor Flow\n",
            " First we present results on established entity linking and entity typing tasks , to compare RELIC 's performance to established baselines and we show that the choice of masking strategy ( Section 3 ) has a significant and opposite impact on performance on these tasks .\n",
            " We evaluate RELIC 's ability to capture entity properties on the FIGMENT and TypeNet entity - level fine typing tasks which contain 102 and 1,077 types drawn from the Freebase ontology .\n",
            " The task in both datasets is to predict the set of fine - grained types that apply to a given entity .\n",
            " Trivia QA ) is a question - answering dataset containing questions sourced from trivia websites , and the answers are usually entities with Wikipedia pages .\n",
            " \n",
            "Important points in entity_linking - 9 :\n",
            "Hence , we can measure the similarity between any pair of items ( i.e. , words , entities , and a word and an entity ) by simply computing their cosine similarity .\n",
            " Textual context similarity is measured according to vector similarity between an entity and words in a document .\n",
            " We then explain our method to construct an embedding that jointly maps words and entities into the same continuous d-dimensional vector space .\n",
            " Inspired by WLM , the KB graph model simply learns to place entities with similar incoming links near one another in the vector space .\n",
            " We replace the log P ( w t+j |w t ) term in Eq. ( 1 ) with the above objective function .\n",
            " We also replace log P ( e o |e i ) in Eq. ( 4 ) and log P ( w o |e i ) in Eq. ( 6 ) in the same manner .\n",
            " Given a set of entity mentions M = {m 1 , m 2 , ... , m N } in a document d with an entity set E = {e 1 , e 2 , ... , e K } in the KB , the task is defined as resolving mentions ( e.g. , \" Washington \" ) into their referent entities ( e.g. , Washington D.C. ) .\n",
            " In candidate generation , candidates of referent entities are generated for each mention .\n",
            " We also add the number of entity candidates for mention m as a feature .\n",
            " The above set of four features is called base features in the rest of the paper .\n",
            " Moreover , as with the corpus used in the embedding , we used the December 2014 version of the Wikipedia dump as the referent KB , and to derive the prior probability as well as the entity prior .\n",
            " \n",
            "Important points in entity_linking - 7 :\n",
            "We then present a semi-supervised algorithm which uses label propagation to label unlabeled sentences based on their similarity to labeled ones .\n",
            " WSD algorithms based on supervised learning are generally believed to perform better than knowledgebased WSD algorithms , but they need large training sets to perform well .\n",
            " Given a window of text w n?k , ... , w n , ... , w n+k surrounding a focus word w n ( whose label is either known in the case of example sentences or to be determined in the case of classification ) , an embedding for the context is computed as a concatenation or weighted sum of the embeddings of the words w i , i = n.\n",
            " Additional heuristics helped to maintain the stability of the bootstrapping process .\n",
            " We therefore directly use the LSTM 's context layer from which the bag of predictions was computed as a representation of the context ( see ) .\n",
            " In addition , they will receive stock in the reorganized company , which will be named Ranger Industries Inc.\n",
            " Sense labels are then propagated from the labeled to the unlabeled sentences .\n",
            " The disagreement between the label distributions of connected vertices .\n",
            " The LSTM model has 2048 hidden units , and inputs are 512 - dimensional word vectors .\n",
            " We train the LSTM model by minimizing sampled softmax loss with Adagrad .\n",
            " compares the F 1 scores of the LSTM and baseline algorithms .\n",
            " \n",
            "Important points in entity_linking - 3 :\n",
            "Semisupervised approaches extend manually created training sets by large corpora of unlabeled data to improve WSD performance .\n",
            " In contrast , the distributional hypothesis not only states that meaning is dependent on context .\n",
            " Still , a polysemic term is represented by one single vector only , which represents all of its different senses in a collapsed fashion .\n",
            " Camacho - Collados and Pilehvar ( 2018 ) provide an extensive overview of different word sense modeling approaches .\n",
            " For CWEs , outputs from the embedding layer , and the two bidirectional recurrent layers are not concatenated , but collapsed into one layer by a weighted , element - wise summation .\n",
            " Self- attention and non-directionality of the language modeling task result in extraordinary performance gains compared to previous approaches .\n",
            " Our approach considers only senses fora target word that have been observed during training .\n",
            " 1 https://spacy.io/\n",
            " We are using Version 2.1 : https://github.com/getalp/\n",
            " UFSAC 4 BERT performed best in experiment one .\n",
            " Example ( 12 ) also emphasizes the difficulty of distinguishing verb senses itself , i.e. the correct sense label in this example is watch % 2:39:00 : : look attentively whereas the predicted label and the nearest neighbor is watch % 2:41:00 : : follow with the eyes or the mind ; observe .\n",
            " \n",
            "Important points in entity_linking - 2 :\n",
            "First , we show that neural sequence learning represents a novel and effective alternative to the traditional way of modeling supervised WSD , enabling a single all - words model to compete with a pool of word experts and achieve state - of - the - art results , while also being easier to train , arguably more versatile to use within downstream applications , and directly adaptable to different languages without requiring additional sense - annotated data ( as we show in Section 6.2 ) ; second , we carryout an extensive experimental evaluation where we compare various neural architectures designed for the task ( and somehow left underinvestigated in previous literature ) , exploring different configurations and training procedures , and analyzing their strengths and weaknesses on all the standard benchmarks for all - words WSD .\n",
            " Similarly , Context2 Vec makes use of a bidirectional LSTM architecture trained on an unlabeled corpus and learns a context vector for each sense annotation in the training data .\n",
            " We use the notation of Navigli ( 2009 ) for word senses : w i p is the i - th sense of w with part of speech p.\n",
            " Training .\n",
            " x 5\n",
            " x into a real - valued vector x i via an embedding layer , and then fed to an encoder , which generates a fixed - dimensional vector representation of the sequence .\n",
            " A memorization task , where the model learns to replicate the input sequence token by token at decoding time ;\n",
            " In particular , we include one or more bidirectional LSTM layers at the core of both the encoder and the decoder modules .\n",
            " Architecture Details .\n",
            " To set a level playing field with comparison systems on English all - words WSD , we followed and , for all our models , we used a layer of word embeddings pre-trained 8 on the English uk WaC corpus as initialization , and kept them fixed during the training process .\n",
            " Back - off strategy aside , 85 % of the times the top candidate sense fora target instance lay within the 10 most probable entries in the probability distribution over O computed by the softmax layer .\n",
            " \n",
            "Important points in entity_linking - 4 :\n",
            "These methods encode information of entities in the KB into a continuous vector space .\n",
            " A representative example is Explicit Semantic Analysis ( ESA ) , which represents the semantics of a text using a sparse vector space , where each dimension corresponds to the relevance score of the text to each entity .\n",
            " As a result , our method clearly outperformed the state - of - the - art methods .\n",
            " We compute v t using the element - wise sum of word vectors int with L 2 normalization and a fully connected layer .\n",
            " It was extracted from the first introductory sections of 4.4 million Wikipedia articles .\n",
            " As a result , our vocabulary V consisted of 705,168 words and 957,207 entities .\n",
            " The training took approximately six days using a NVIDIA K80 GPU .\n",
            " Finally , we further qualitatively analyze the learned representations .\n",
            " Entity Linking ( EL ) is the task of resolving ambiguous mentions of entities to their referent entities in KB .\n",
            " EL has recently received considerable attention because of its effectiveness in various NLP tasks such as information extraction and semantic search .\n",
            " Additionally , we used a vector filled with zeros as representations of entities that were not contained in our vocabulary .\n",
            " \n",
            "Important points in entity_linking - 6 :\n",
            "The gloss , which extensionally defines a word sense meaning , plays a key role in the well - known Lesk algorithm .\n",
            " the related glosses of hypernyms and hyponyms into the neural network .\n",
            " We further expand the gloss through its semantic relations to enrich the gloss information and better infer the context .\n",
            " Recent researches show that memory network obtains the state - of - the - art results in many NLP tasks such as sentiment classification and analysis , poetry generation , spoken language understanding , etc .\n",
            " Then , the forward LSTM reads the segment ( x 1 , . . . , x t?1 ) on the left of the target word x t and calculates a sequence of forward hidden states\n",
            " The scoring module calculates the scores for all the related senses {s 1 t , s 2 t , . . . , s pt } corresponding to the target word x t and finally outputs a sense probability distribution overall senses .\n",
            " where ?\n",
            " Gloss Module\n",
            " Like the context encoder , the gloss encoder also leverages Bi - LSTM units to process the words sequence of the gloss .\n",
            " The gloss representation g i j is computed as the concatenation of the last hidden states of the forward and backward LSTM .\n",
            " Dataset\n",
            " \n",
            "Important points in entity_linking - 13 :\n",
            "We aim to mitigate these problems by ( 1 ) modeling the sequence of words surrounding the target word , and ( 2 ) refrain from using any hand crafted features or external resources and instead represent the words using real valued vector representation , i.e. word embeddings .\n",
            " A WSD system that , by using no explicit window , is allowed to combine local and global information when deducing the sense .\n",
            " The bidirectional variant of LSTM , ( BLSTM ) is an adaptation of the LSTM where the state at each time step consist of the state of two LSTMs , one going left and one going right .\n",
            " ] is the concatenated outputs of the right and left traversing LSTMs of the BLSTM at word n.\n",
            " i which is the standard deviation in embedding dimension i for all words in the embeddings matrix , W x . ?\n",
            " The only preprocessing of the data that is conducted is replacing numbers with a < number > tag .\n",
            " However , it also relies on a rich set of other features including POS tags , collocations and surrounding words to achieve their reported result .\n",
            " F1 score\n",
            " \n",
            "Important points in entity_linking - 14 :\n",
            "In order to model the whole document for WSD , we leverage the formalism of topic models , especially Latent Dirichlet Allocation ( LDA ) .\n",
            " Related Work\n",
            " However , Chaplot , show that sense of a word depends on not just the words in the context but also on their senses .\n",
            " Word Net\n",
            " Semantics\n",
            " some words in each synset are more frequent than others : modeled using non-uniform priors for the synset distribution over words .\n",
            " Thus , we hypothesize that the meaning of the word depends on words outside the sentence in which it occurs - as a result , we use the whole document containing the word as its context .\n",
            " It is based on the key assumption that documents exhibit multiple topics ( which are nothing but distributions over some fixed vocabulary ) .\n",
            " parameter is used to model the relationship between synsets .\n",
            " Since , the inverse of covariance matrix will be used in inference , we directly choose ( i , j) th element of inverse of covariance matrix as follows : ? ? 1\n",
            " Firstly , LDA requires the specification of the number of topics as a hyper - parameter which is difficult to tune .\n",
            " \n",
            "Important points in entity_linking - 16 :\n",
            "Finally , in Section 5 , we conclude with some future research directions for the construction of sense embeddings as well as applications of such model in other domains such as biomedicine .\n",
            " Neural Embeddings for WSD\n",
            " Chen et al .\n",
            " ( 1 ) will have the effect of picking the column ( i.e. sense embeddings ) from W l s corresponding to that sense .\n",
            " Also , all weights including embeddings are updated during training .\n",
            " This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass , and any weight updates are not applied to the neuron on the backward pass .\n",
            " The preprocessing of the data was conducted by lower - casing all the words in the documents and removing numbers .\n",
            " Between - all - models comparisons\n",
            " That is to say , the separate spaces that the sense embeddings and the ( context ) word embeddings come from enforces some delay for the alignment of these spaces which in turn demands more training data .\n",
            " Furthermore , this early misalignment does not allow the BLSTM fully take advantage of larger context sizes which can be helpful .\n",
            " \n",
            "Important points in entity_linking - 15 :\n",
            "The QA datasets are normally collected from the web and contain very noisy and diverse data , which poses a number of challenges for EL .\n",
            " Each level of granularity is handled by a separate component of the model .\n",
            " To the best of our knowledge , we are the first to present a unified end - to - end neural model for entity linking for noisy data that operates on different context levels and does not rely on manual features .\n",
            " Named entity recognizers are normally trained to detect mentions of Locations , Organizations and Person names , whereas in the context of QA , the system also needs to cover movie titles , songs , common nouns such as ' president ' etc .\n",
            " The overall architecture of our entity linking system is depicted in .\n",
            " Step 2 . entity candidates for an n -gram C = entity candidates ( n )\n",
            " That ensures that the top candidates in the list would be those that exactly match the target n-gram n.\n",
            " The neural architecture ( Variable Context Granularity , VCG ) aggregates and mixes contexts of different granularities to perform a joint mention detection and entity disambiguation .\n",
            " The entity embeddings are transformed by a fully - connected layer f e and then also pooled to produce the output o e .\n",
            " The embedding of the candidate entity itself is also transformed with f e and is stored as o d .\n",
            " We extract all entities that are mentioned in the question from the SPARQL query .\n",
            " \n",
            "Important points in entity_linking - 5 :\n",
            "In general , a word can be disambiguated using the hypernyms of its senses , and not necessarily the senses themselves .\n",
            " Our method is then evaluated by measuring its contribution to a state of the art neural WSD system evaluated on classic WSD evaluation campaigns .\n",
            " At test time , the language model is used to predict a vector according to the surrounding context , and the sense closest to the predicted vector is assigned to each word .\n",
            " The training of systems that directly predict a tag in the set of all WordNet senses becomes slower and take more memory the larger the output vocabulary is .\n",
            " For the sake of WSD , just like grouping together the senses of a same synset helps to better generalize , we hypothesize that grouping together the synsets of a same hypernymy relationship also helps in the same way .\n",
            " This is illustrated in .\n",
            " We transform our sense vocabulary by mapping every synset to the lowest synset in its hypernymy hierarchy that is marked as \" necessary \" .\n",
            " Note that if we narrow down our computation to consider only polysemic words in WordNet , the full vocabulary of all reduced synsets of WordNet is 23 148 , and the SemCor contains 9 461 of them represented , and that is a coverage of approximately 40 % .\n",
            " Training\n",
            " We compared our sense vocabulary reduction method on two training sets :\n",
            " On the total of 7 253 words to annotate for the corpus \" ALL \" , the baseline system trained on the SemCor only is incapable of annotating 491 of them , and with the vocabulary reduction applied this number drops to 91 .\n",
            " \n",
            "Important points in entity_linking - 1 :\n",
            "We evaluated the proposed model by addressing NED using an NED model based on trained contextualized embeddings .\n",
            " Such embedding models enable us to design NED models that capture the contextual information required to address NED .\n",
            " Given a sequence of tokens consisting of words and entities , the model first represents the sequence as a sequence of input embeddings , one for each token , and then the model generates a contextualized output embedding for each token .\n",
            " We also set the feed - forward / filter size to 4096 , the dropout probability applied to all layers was 0.1 , and the maximum word length in an input sequence was set to 512 .\n",
            " Pseudo entity annotations are created by treating all mentions except the target mention in the document as entity annotations referring to their entity candidates .\n",
            " Note that the contextual information obtained from entities appearing in the same document has been considered as critical in improving NED and a main focus of the past literature on NED .\n",
            " During the training , we fixed the entity token embeddings ( B and B * ) and output bias ( b o and b * o ) , and updated all other parameters .\n",
            " Results\n",
            " \n",
            "Important points in entity_linking - 10 :\n",
            "The addition of ELMo representations alone significantly improves the state of the art in every case , including up to 20 % relative error reductions .\n",
            " Previously proposed methods overcome some of the shortcomings of traditional word vectors by either enriching them with subword information ( e.g. , or learning separate vectors for each word sense ( e.g. , .\n",
            " 1 http://allennlp.org/elmo\n",
            " For inclusion in a downstream model , ELMo collapses all layers in R into a single vector , ELMo k = E(R k ; ? e ) .\n",
            " In contrast , traditional word embedding methods only provide one layer of representation for tokens in a fixed vocabulary .\n",
            " Once pretrained , the biLM can compute representations for any task .\n",
            " Our baseline model is an improved version of the Bidirectional Attention Flow model in .\n",
            " Due to the small test sizes for NER and SST - 5 , we report the mean and standard deviation across five runs with different random seeds .\n",
            " Previous work on contextual representations used only the last layer , whether it be from a biLM or an .\n",
            " The choice of the regularization parameter is also important , as large values such as = 1 effectively reduce the weighting function to a simple average over the layers , while smaller values ( e.g. , = 0.001 ) allow the layer weights to vary .\n",
            " As the linear classifier adds only a small amount of model capacity , this is direct test of the biLM 's representations .\n",
            " \n",
            "Important points in entity_linking - 11 :\n",
            "Contributions : In this paper , we hypothesize that only a subset of WordNet senses could be considered to disambiguate all words of the lexical database .\n",
            " Multiple works also exploit the idea of grouping together related senses .\n",
            " WSD Based on a Softmax Classifier\n",
            " Sense Vocabulary\n",
            " Indeed , considering a small subset of WordNet that only consists of the word \" mouse \" , its first sense ( the small rodent ) , its fourth sense ( the elec - We computed that 41 607 on the 44 449 polysemous nouns of WordNet ( 94 % ) are part of this hierarchy .\n",
            " We can see that every concept that is more specialized than the concepts \" artifact \" and \" living _ thing \" could be removed .\n",
            " Compression through all semantic relationships\n",
            " First , we initialize the set of clusters C with one synset in each cluster .\n",
            " The development set was composed of 4 000 random sentences taken from the Princeton WordNet Gloss Corpus for the models trained on the Sem - Cor , and 4 000 random sentences extracted from the whole training set for the other models .\n",
            " For each training set , we trained three systems :\n",
            " 5 https://allennlp.org/elmo\n",
            " \n",
            "Important points in entity_linking - 0 :\n",
            "Entity Embeddings .\n",
            " We present a novel attention mechanism for local ED .\n",
            " Ina similar local setting , embed mentions , their immediate contexts and their candidate entities using word embeddings and CNNs .\n",
            " These counts define a practical approximation of the above word - entity conditional distribution , i.e.p ( w |e ) ? # ( w , e ) .\n",
            " c and entity e ? ? ( m ) is mapped to its embedding via the pre-trained map x ( cf. Section 3 ) .\n",
            " The weight is high if the word is strongly related to at least one candidate entity .\n",
            " We therefore use a neural network with two fully connected layers of 100 hidden units and ReLU non-linearities , which we regularize as suggested in by constraining the sum of squares of all weights in the linear layer .\n",
            " Entity and word embeddings are pre-trained as discussed in Section 3 .\n",
            " Herein the first part just reflects the CRF potentials , whereas the second part is defined as\n",
            " where ? ?\n",
            " We first train each entity vector on the entity 's Wikipedia canonical description page ( title words included ) for 400 iterations .\n",
            " \n",
            "Important points in entity_linking - 12 :\n",
            "The gloss , which extensionally defines a word sense meaning , plays a key role in the well - known Lesk algorithm .\n",
            " In this paper , we propose a novel model GAS : a gloss - augmented WSD neural network which is a variant of the memory network .\n",
            " We extend the gloss module in GAS to a hierarchical framework in order to mirror the hierarchies of word senses in WordNet .\n",
            " Inspired by the success of memory network used in many NLP tasks , we introduce it into WSD .\n",
            " The backward LSTM reads the segment ( x Tx , . . . , x t+1 ) on the right of the target word x t and calculates a sequence of backward hidden states ( ? ? h Tx , . . . , ? ? h t+1 ) .\n",
            " The scoring module calculates the scores for all the related senses {s 1 t , s 2 t , . . . , s pt } corresponding to the target word x t and finally outputs a sense probability distribution overall senses .\n",
            " where ?\n",
            " Gloss Module\n",
            " the expanded glosses list for i th sense of the target word as a sequence of G words .\n",
            " Like the context encoder , the gloss encoder also leverages Bi - LSTM units to process the words sequence of the gloss .\n",
            " Evaluation Dataset : we evaluate our model on several English all - words WSD datasets .\n",
            " \n",
            "Important points in hypernym_discovery - 7 :\n",
            "offer an implementation that is efficient for gigaword corpora .\n",
            " Our source code is available online 1 .\n",
            " ( There is a closure - operator related to each FCA context , for which O and A are closed sets iff O , A is a concept . )\n",
            " Finally , we refer to the set of basis vectors ( in the FCA terminology , attributes ) which are assigned non -zero weights in the reconstruction of the vectorial representation of q and h as ? ( q ) and ?( h ) .\n",
            " Both submissions used the conceptually motivated but practically harmful FCA - based features .\n",
            " For reproducibility , we report result without the isFreqHyp feature .\n",
            " For this reason - even though our official shared task submission included FCArelated features - we no longer employed them in our post-evaluation experiments .\n",
            " We call this strategy as candidate filtering .\n",
            " \n",
            "Important points in hypernym_discovery - 8 :\n",
            "Moreover , some taxonomy learning approaches link their concepts to existing resources such as Wikipedia .\n",
            " 3 Universal Dependencies ( UD ) is a framework for crosslinguistically consistent grammatical annotation and an open community effort with over 200 contributors producing more than 100 treebanks in over 60 languages .\n",
            " Dependency parsing : the syntactic parsing of a sentence consists of finding the correct syntactic structure of that sentence in a given formalism / grammar .\n",
            " While some relations have not been very fruitful ( such as X \" obj \" Y , for insance ) , others , instead , have been very productive , generating tens of thousands relations .\n",
            " \n",
            "Important points in hypernym_discovery - 6 :\n",
            "They reformulate the task from hypernym detection into hypernym discovery .\n",
            " The data contains a list of training terms along with gold hypernyms , a list of testing terms , and a vocabulary search space .\n",
            " Path - Based\n",
            " Candidate Hypernyms\n",
            " Next , we represent each term and its candidate hypernym by a concatenated feature vector .\n",
            " The training dataset was unbalanced , the ratio of hypernym instances w.r.t. not hypernym is less than 0.05 .\n",
            " In other words , our system lacks the ability to discover hypernyms for entity terms .\n",
            " The second reason is that some conditions used to extract candidate hypernyms restrict the number of candidate hypernyms .\n",
            " \n",
            "Important points in hypernym_discovery - 5 :\n",
            "Furthermore , we discover additional hypernyms using a method based on the following assumptions : most multi-word expressions are compositional , and the prevailing head - modifier relation is hypernymy .\n",
            " 3 .\n",
            " 6 .\n",
            " R d 1 using a lookup table .\n",
            " For regularization , we also use gradient clipping , as well as early stopping .\n",
            " We sample positive examples using a function based on the frequency of the hypernyms in the training data , such that we subsample ( q , h) pairs where h occurs often in the training data .\n",
            " Data Augmentation\n",
            " Given a positive example ( q , h ) ? D , add ( q , h) to the positive examples , where q is the nearest neighbour of q , based on the cosine similarity of the embeddings of all the words in the vocabulary .\n",
            " Given this observation , we find it somewhat surprising that run 1 is the best on all 3 test sets when we use the hybrid system .\n",
            " One possible explanation is that adding the synthetic examples makes the errors of the supervised system more different from those of the unsupervised system , and that this in turn makes the ensemble method more beneficial , but we have n't looked into this .\n",
            " The case of vegetarian suggests that syntactic ambiguity is a source of errors : the predicted hypernyms include some that might be considered valid for the query vegetarian food , where vegetarian is an adjective , but not for the noun vegetarian .\n",
            " \n",
            "Important points in hypernym_discovery - 0 :\n",
            "Although these word embeddings do not distinguish one semantic relation from another , we expect that true hypernyms will constitute a significant proportion of the predicted candidate hypernyms .\n",
            " Section 5 concludes and proposes avenues for future work .\n",
            " The negative sampling portion of the algorithm is away of producing \" negative \" context words for the focus word by simply drawing random words from the corpus .\n",
            " On the music industry domain subtask , our system ranked 13th out of 16 places with a MAP of 1.88 , ranking 4th among the unsupervised systems .\n",
            " \n",
            "Important points in hypernym_discovery - 4 :\n",
            "Variations of were later used in tasks such as taxonomy construction ; ; ) , analogy identification ( Turney ( 2006 ) ) , and definition extraction ; Navigli and Velardi ( 2010 ) ) .\n",
            " However , it was criticized that the supervised methods only learn prototypical hypernymy ) .\n",
            " For example , \" executive president \" and \" vice executive president \" both exist in the corpus sentence \" Hoang Van Dung , vice executive president of the Vietnam Chamber of Commerce and Industry . \" .\n",
            " 3 .\n",
            " The training set consists of the negative pairs and the positive pairs in 3:1 ratio .\n",
            " k x , y) /?\n",
            " This proves the effectiveness of the method .\n",
            " We leave this to the future work .\n",
            " \n",
            "Important points in hypernym_discovery - 3 :\n",
            "In this area , supervised approaches , arguably the most popular nowadays , learn a feature vector between term - hypernym vector pairs and train classifiers to predict hypernymic relations .\n",
            " In addition to pattern - based , other terms like path - based or rule - based are also used .\n",
            " One of the main drawbacks of these methods is that they require both term and hypernym to co-occur in text within a certain window , which strongly hinders their recall .\n",
            " Wikidata 5 ) is a document - oriented semantic data base operated by the Wikimedia Foundation with the goal of providing a common source of data that can be used by other Wikimedia projects .\n",
            " Domain Clustering\n",
            " Finally , given a BabelNet synset b , we calculate the similarity between its corresponding NASARI lexical vector and all the domain vectors , selecting the domain leading to the highest similarity score :\n",
            " For each term - hypernym pair , both concepts are expanded to their given lexicalizations and thus , each synset pair term - hypernym in the training data is expanded to a set of | L t |.| L h | sense training pairs .\n",
            " The gist of our approach lies on the property of current semantic vector space models to capture relations between vectors , in our case hypernymy .\n",
            " Results and discussion\n",
            " In fact , the overlap between our approach and the remaining systems is actually quite small ( on average less than 25 % with all of them on the Extra - Coverage experiment ) .\n",
            " \n",
            "Important points in hypernym_discovery - 2 :\n",
            "We demonstrate that since each of these measures captures a different aspect of the hypernymy relation , there is no single measure that consistently performs well in discriminating hypernymy from different semantic relations .\n",
            " Distributional Semantic Spaces\n",
            " the contexts of a target word w i are the words surrounding it in a ksized window : w i?k , ... , w i ?1 , w i + 1 , ... , w i+k .\n",
            " Cosine Similarity )\n",
            " SLQS Sub\n",
            " This is achieved by subtraction :\n",
            " Reversed Inclusion Measures\n",
            " Reversed Weeds\n",
            " The inclusion hypothesis seems to be most effective in discriminating between hypernyms and meronyms under syntactic contexts .\n",
            " We conjecture that the windowbased contexts are less effective since they capture topical context words , that might be shared also among holonyms and their meronyms ( e.g. car will occur with many of the neighbors of wheel ) .\n",
            " v y , difference v y ? v x , and ASYM .\n",
            " \n",
            "Important points in hypernym_discovery - 1 :\n",
            "In this work , we introduce a neural network architecture for the concerned task and empirically study various neural networks to model the distributed representations for words and phrases .\n",
            " We first train term embeddings , either using word embedding or sense embedding to represent each word .\n",
            " Hypernym Learning\n",
            " RCNN\n",
            " We run all our models up to 50 epoch and select the best result in validation . :\n",
            " Result and analysis\n",
            " In this paper , we introduce a neural network architecture for the hypernym discovery task and empirically study various neural network models to model the representations in latent space for words and phrases .\n",
            " \n",
            "Important points in dependency_parsing - 6 :\n",
            "A dependency tree parse for Casey hugged Kim , including part - of - speech tags and a special root token .\n",
            " present a neural graph - based parser ( in addition to a transition - based one ) that uses the same kind of attention mechanism as for machine translation .\n",
            " In addition to having one bidirectional recurrent network that computes a recurrent hidden vector for each word , they have additional , unidirectional recurrent networks ( leftto - right and right - to - left ) that keep track of the probabilities of each previous arc , and use these together to predict the scores for the next arc .\n",
            " HYPERPARAMETER CONFIGURATION\n",
            " Because we increase the parser 's power , we also have to increase its regularization .\n",
            " Models trained with only word or tag dropout but not both windup signficantly overfitting , hindering label accuracy and - in the latter case - attachment accuracy .\n",
            " Where our model appears to lag behind the SOTA model is in LAS , indicating one of a few possibilities .\n",
            " CONCLUSION\n",
            " \n",
            "Important points in dependency_parsing - 7 :\n",
            "We do not describe the model in detail , and refer the reader to the original work .\n",
            " Regardless of the oracle , our training implementation constructs a computation graph ( nodes that represent values , linked by directed edges from each function 's inputs to its outputs ) for the negative log probability for the oracle transition sequence as a function of the current model parameters and uses forward - and backpropagation to obtain the gradients respect to the model parameters .\n",
            " Instead , we would prefer to train the parser to behave optimally even after making a mistake ( under the constraint that it can not backtrack or fix any previous decision ) .\n",
            " Taking this idea further , we could increase the number of error-states observed in the training process by changing the sampling distribution so as to bias it toward more low - probability states .\n",
            " 3 .\n",
            " Training greedy parsers on non-gold outcomes , facilitated by dynamic oracles , has been explored by several researchers in different ways\n",
            " \n",
            "Important points in dependency_parsing - 5 :\n",
            "However , although their model outperforms its greedy hand - engineered counterparts , it is not competitive with state - of - the - art dependency parsers thatare trained for structured search .\n",
            " However , we do not use these probabilities directly for prediction .\n",
            " We provide an extensive exploration of our model in Section 5 through ablative analysis and other retrospective experiments .\n",
            " Input layer\n",
            " We thus present a novel way to leverage a neural network representation in a structured prediction setting .\n",
            " In this work , we investigate a semi-supervised structured learning scheme that yields substantial improvements in accuracy over the baseline neural network model .\n",
            " Pretraining\n",
            " is a regularization hyper - parameter over the hidden layer parameters ( we use ? = 10 ? 4 in all experiments ) and j sums over all decisions and configurations {y j , c j } extracted from gold parse trees in the dataset .\n",
            " On each training example , we run beam search until the goldstandard parse tree falls out of the beam .\n",
            " 1 Define j to be the length of the beam at this point .\n",
            " For our tri-training experiments , we re-train the POS tagger using the POS tags assigned on the unlabeled data from the Berkeley constituency parser .\n",
            " \n",
            "Important points in dependency_parsing - 8 :\n",
            "We also outperform despite using a smaller beam .\n",
            " To apply it to different tasks we only need to adjust the transition system and the input features .\n",
            " A ( s , x ) .\n",
            " Beam search can be used to attempt to find the maximum of Eq .\n",
            " The theorem was originally proved 5 by .\n",
            " First , consider a locally normalized model where we restrict the scoring function to access only the first i input symbols x 1:i when scoring decision d i .\n",
            " P L is a strict subset of PG , that is P LP G .\n",
            " The proof that PG P L gives a clear illustration of the label bias problem .\n",
            " For every amount of lookahead k , we can construct examples that can not be modeled with a locally normalized model by duplicating the middle input bin ( 7 ) k + 1 times .\n",
            " Only a local model with scores ?( d 1:i? 1 , d i , x 1:n ) that considers the entire input can capture any distribution p ( d 1:n | x 1 :n ) : in this case the decomposition p L ( d 1:n | x 1:n ) = n i = 1 p L ( d i |d 1:i? 1 , x 1:n ) makes no independence assumptions .\n",
            " It additionally also has transition features of the word , cluster and character n-gram up to length 3 on both endpoints of the transition .\n",
            " \n",
            "Important points in dependency_parsing - 3 :\n",
            "Despite this , that work , and most other related work , has simply adopted a tool to analyze the syntactic characteristics of the biomedical texts without consideration of the appropriateness of the tool for these texts .\n",
            " Event extraction few years , thanks to renewed attention to the problem and exploration of neural methods , it is important to revisit whether the commonly used tools remain the best choices for syntactic analysis of biomedical texts .\n",
            " In particular , we compare pre-trained and retrained POS taggers , and investigate the effect of these pre-trained and retrained taggers in pre-trained parsing models ( in the first five rows of ) .\n",
            " http://bionlp-corpora.sourceforge.net/CRAFT\n",
            " We use the Stanford Biaffine parser v 2 in our experiments .\n",
            " Then we report final evaluation results on the test set .\n",
            " We also fix the number of BiLSTM layers at 2 and select the number of LSTM units in each layer from { 100 , 150 , 200 , 250 , 300 } .\n",
            " POS tagging accuracies on the test set with gold tokenization .\n",
            " The first two rows present scores of the pre-trained Stanford NNdep and Biaffine v 1 models with POS tags predicted by the pre-trained Stanford tagger , while the next two rows 3 - 4 present scores of these pretrained models with POS tags predicted by NLP4J - POS .\n",
            " Both pre-trained NNdep and Biaffine models were trained on a dependency treebank of 40K sentences , which was converted from the English PTB sections 2 - 21 .\n",
            " Sentence length\n",
            " \n",
            "Important points in dependency_parsing - 0 :\n",
            "Furthermore , experimental results on downstream task applications show that our joint model helps produce state - of - the - art scores for biomedical event extraction and opinion analysis .\n",
            " These latent feature representations are fed into a MLP to decode dependency arcs and another MLP to label the predicted dependency arcs .\n",
            " We feed the sequence of vectors e 1:n with an additional context position index i into another BiL - STM ( BiLSTM pos ) , resulting in latent feature vectors v\n",
            " Our parsing component can be viewed as an extension of the BIST graph - based dependency model , where we additionally incorporate the character - level vector representations of words .\n",
            " Experiments on English Penn treebank\n",
            " We follow a standard data split to use sections 02 - 21 for training , Section 22 for development and Section 23 for test , employing the Stanford conversion toolkit v 3.3.0 to generate dependency trees with Stanford basic dependencies .\n",
            " Other differences are that they use a higher dimensional representation than ours , but rely on predicted POS tags .\n",
            " The 82 treebanks are taken from UD v 2.2 , where 61 / 82 test sets are for \" big \" UD treebanks for which both training and development data sets are available and 5 / 82 test sets are extra \" parallel \" test sets in languages where another big treebank exists .\n",
            " The EPE 2018 campaign runs in collaboration with the CoNLL 2018 shared task , which aims to evaluate dependency parsers by comparing their performance on three downstream tasks : biomedical event extraction , negation resolution and opinion analysis .\n",
            " Here , participants only need to provide parsing outputs of English raw texts used in these downstream tasks ; the campaign organizers then compute end - to - end downstream task .\n",
            " \n",
            "Important points in dependency_parsing - 1 :\n",
            "In this work , we suggest an approach which is much simpler in terms of both feature engineering and architecture engineering .\n",
            " In the graphbased parser , we jointly train a structured - prediction model on top of a BiLSTM , propagating errors from the structured objective all the way back to the BiLSTM feature - encoder .\n",
            " We use x 1:n to denote a sequence of n vectors x 1 , , x n . F ? ( ) is a function parameterized with parameters ?.\n",
            " Titov and Henderson encode the parser state using incremental sigmoid - belief .\n",
            " Each output vector hi is conditioned on all the input vectors x 1 :i , and can bethought of as a summary of the prefix x 1:i of x 1:n .\n",
            " Concretely , given a sequence of vectors x 1:n and a desired index i , the function BIRNN ? ( x 1 :n , i ) is defined as :\n",
            " The training procedure causes the BiRNN function to extract from the input sequence x 1:n the relevant information for the task task at hand .\n",
            " Historical Notes RNNs were introduced by Elman , and extended to BiRNNs by .\n",
            " After a finite number of transitions , the system arrives at a terminal configuration , and a parse tree is read off the terminal configuration .\n",
            " In a greedy parser , a classifier is used to choose the transition to take in each configuration , based on features extracted from the configuration itself .\n",
            " 7\n",
            " \n",
            "Important points in dependency_parsing - 4 :\n",
            "Incorporating this global search algorithm with distributed representations learned from neural networks , neural graph - based parsers have achieved the state - of - the - art accuracies on a number of treebanks in different languages .\n",
            " We evaluate our parser on 29 treebanks across 20 languages and different dependency annotation schemas , and achieve state - of - the - art performance on 21 of them .\n",
            " shows a dependency tree for the sentence , \" But there were no buyers \" .\n",
            " The parser chooses a specific position c according to the attention scores in at to generate a new dependency arc ( w h , w c ) by selecting w c as a child of w h .\n",
            " To utilize higher - order information , the decoder 's input at each step is the sum of the encoder hidden states of three words :\n",
            " illustrates the details .\n",
            " represents model parameters .\n",
            " Arc Prediction\n",
            " We use the same hyper - parameters across the models on different treebanks and languages , due to time constraints .\n",
            " The details of the chosen hyper - parameters for all experiments are summarized in Appendix A.\n",
            " Following McDonald and Nivre ( 2011 ) , we analyze parsing errors related to structural factors .\n",
            " \n",
            "Important points in dependency_parsing - 2 :\n",
            "The code to reproduce our results is publicly available .\n",
            " where T ( x ) is the set of directed trees over x , and sis a local scoring function that considers only a single dependency arc at a time .\n",
            " Let h y ( m ) denote the parent of x min y ( using a special null symbol when m is the root of the tree ) , and h y ( m ) denotes the parent of x min the predicted tree y .\n",
            " We can therefore view s as a posterior marginal , and the ensemble parser as an MBR parser ( Eq. 2 ) .\n",
            " The idea is attractive because cost functions are model - agnostic ; they can be used with any parser amenable to discriminative training .\n",
            " In their work , the ensemble provides a probability distribution over labels for each input , and this predicted distribution serves as the training target for the distilled model ( a sum of two cross entropies objective is used , one targeting the empirical training distribution and the other targeting the ensemble 's posterior distribution ) .\n",
            " When incorporated into discriminative training , the Hamming cost encodes hard targets : the correct attachment should receive a higher score than all incorrect ones , with the same margin .\n",
            " This function has several attractive properties :\n",
            " The bidirectional LSTM maps each word xi to a vectorx i that embeds the word in context ( i.e. , x 1:i?1 and x i + 1:n ) .\n",
            " Local attachment scores are given by :\n",
            " The model trained with Hamming cost achieved 93.1 UAS and 90.9 LAS , compared to 93.6 UAS and 91.1 LAS for the model with distillation cost .\n",
            " \n",
            "Important points in data-to-text_generation - 2 :\n",
            "They rely on recurrent data encoders with memory and gating mechanisms ( LSTM ; ) .\n",
            " The main contribution of this work is show - :\n",
            " The specific semantics of X depends on the task at hand .\n",
            " In this way , each GCN layer is directly fed with the output of every layer before itself .\n",
            " Nodes in the graph are lemmas of the target sentence .\n",
            " 5 Each node is also associated with morphological ( e.g. num=sg ) and punctuation features ( e.g. bracket = r ) .\n",
            " For the SR11 Deep baseline we follow a similar linearis ation procedure as proposed for AMR graphs .\n",
            " For the SR11 Deep task we used the same architecture with 500 - dimensional hidden states and embeddings .\n",
            " The economy 's temperature will betaken from several vantage points this week , with readings on trade , output , housing and inflation .\n",
            " Baseline the economy 's accords will betaken from several phases this week , housing and inflation readings on trade , housing and inflation .\n",
            " A.2 More example outputs\n",
            " \n",
            "Important points in data-to-text_generation - 3 :\n",
            "Example outputs of our systems on the E2E generation task .\n",
            " In this work we show that they can be extended to a distinct class of language generation problems that use as referents structured descriptions of lingustic content , or other natural language texts .\n",
            " We focus on two conditional generation tasks where the information in the input context should largely be preserved in the output text , and apply the pragmatic procedures outlined in Sec. 3 to each task .\n",
            " where ?\n",
            " We evaluate on the CNN / Daily Mail summarization dataset , using non-anonymized preprocessing .\n",
            " We also report two extractive baselines : Lead - 3 , which uses the first three sentences of the document as the summary , and Inputs , the concatenation of the extracted sentences used as inputs to our models ( i.e. , i ( 1 ) , . . . , i ( P ) ) .\n",
            " While SD 1 optimizes less explicitly for attribute mentions than S R 1 , it still provides a potential method to control generated outputs by choosing alternate distractors .\n",
            " Conclusion\n",
            " \n",
            "Important points in data-to-text_generation - 6 :\n",
            "In order to give an original contribution to the field , in this paper we present a character - level sequence - to - sequence model with attention mechanism that results in a completely neural end - to - end architecture .\n",
            " Finally , in section 4 some conclusions are drawn , outlining future work .\n",
            " which is parameterized as a feedforward neural network and scores how well input in position j- th and output observed in the i - th time instant match ; T x and Ty are the length of the input and output sequences , respectively .\n",
            " where ?\n",
            " In other words , a slot like name , that could virtually contain a very broad range of different values , is filled alternating between 19 fixed possibilities .\n",
            " New values for food are picked from Wikipedia 's list of adjectival forms of countries and nations 1 , while both name and near are filled with New York restaurants ' names contained in the Entree dataset presented in .\n",
            " We developed our system using the PyTorch framework 2 , release 0.4.1 3 .\n",
            " We also propose a new formulation of P ( c ) that helps the model to learn when it is necessary to start a copying phase :\n",
            " Therefore , we can claim that our model exploits its transfer learning capabilities effectively , showing very good performances in a context like data - to - text generation in which the portability of features learned from different datasets , in the extent of our knowledge , has not yet been explored .\n",
            " We highlight that EDA_CS 's model 's good results are achieved even if it consists in a fully end - to - end model which does not benefit from the delexicalizationrelexicalization procedure , differently from TGen .\n",
            " \n",
            "Important points in data-to-text_generation - 4 :\n",
            "Automatic and human evaluation shows that modeling content selection and planning improves generation considerably over competitive baselines .\n",
            " Boston ( 5 - 4 ) has had to deal with a gluttony of injuries , but they had the fortunate task of playing a team just as injured here .\n",
            " Earlier work on content planning has relied on generic planners , based on Rhetorical Structure Theory .\n",
            " Record Encoder\n",
            " The order in the plan corresponds to the sequence in which entities appear in the game summary .\n",
            " Given the input records , the probability p ( z | r ) is decomposed as :\n",
            " We use the encoder - decoder architecture with an attention mechanism to compute p ( y|r , z ) .\n",
            " At decoding step t , the input of the LSTM unit is the embedding of the previously predicted wordy t?1 .\n",
            " We extracted content plans from the ROTOWIRE game summaries following an information extraction ( IE ) approach .\n",
            " Specifically , we used the IE system introduced in Wiseman et al . which identifies candidate entity ( i.e. , player , team , and city ) and value ( i.e. , number or string ) pairs that appear in the text , and then predicts the type ( aka relation ) of each candidate pair .\n",
            " The results of the oracle system ( NCP + OR ) show that content selection and ordering do indeed correlate with the quality of the content plan and that any improvements in our planning component would result in better output .\n",
            " \n",
            "Important points in data-to-text_generation - 5 :\n",
            "2 . John , who works for IBM , was born in London .\n",
            " John works for IBM .\n",
            " He was born in London .\n",
            " The reference can be either a single sentence or a sequence of sentences .\n",
            " Additionally , for each sentence , the plan captures ( 1 ) the ordering of facts within the sentence ; ( 2 ) The ordering of entities within a fact , which we call the direction of the relation .\n",
            " London England A text plan is modeled as a sequence of sentence plans , to be realized in order .\n",
            " Sibling and chain structures can be combined ( ) .\n",
            " While the input RDFs and references are present in the training dataset , the plans are not .\n",
            " That is , we keep track of probabilities such asp s ( s = [ 3 , 2 , 2 ] | 7 ) of realizing an input of 7 RDF triplets as three sentences , each realizing the corresponding number of facts .\n",
            " Relation transitions\n",
            " This allows the NMT system to copy such entities from the input rather than generating them .\n",
            " \n",
            "Important points in data-to-text_generation - 1 :\n",
            "New corpusbased approaches emerged that used semantically aligned data to train language models that output utterances directly from their MRs .\n",
            " We use automatic evaluation metrics to show that these methods appreciably improve the performance of our model .\n",
            " The end - to - end approach to NLG typically requires a mechanism for aligning slots on the output utterances : this allows the model to generate Our work builds upon the successful attentional encoder - decoder framework for sequenceto - sequence learning and expands it through ensembling .\n",
            " Average number of sentences in the reference utterance for a given number of slots in the corresponding MR , along with the proportion of MRs with specific slot counts . cal Turk ( AMT ) , one utterance per MR .\n",
            " Our training data is inherently unaligned , meaning our model is not certain which sentence in a multisentence utterance contains a given slot , which limits the model 's robustness .\n",
            " Applications of our slot aligner are described in subsequent sections and in .\n",
            " Data Preprocessing\n",
            " The placeholders are eventually replaced in the output utterance in postprocessing by copying the values from the input MR .\n",
            " There is a noticeable proportion of those , so we leave them in the training set with the unaligned slots removed from the MR so as to avoid confusing the model when learning from such samples .\n",
            " Sentence Planning via\n",
            " Systems which score similarly according to these metrics could produce utterances thatare significantly different because automatic metrics fail to capture many of the characteristics of natural sounding utterances .\n",
            " \n",
            "Important points in data-to-text_generation - 0 :\n",
            "design a more complex two - step decoder : they first generate a plan of elements to be mentioned , and then condition text generation on this plan .\n",
            " To the best of our knowledge , only Liu et al.\n",
            " Our contribution is threefold :\n",
            " Puduppully et al. follow entity - centric theories and propose a model based on dynamic entity representation at decoding time .\n",
            " For each data - structure sin D , the objective function aims to generate a description ?\n",
            " where ?\n",
            " To keep the semantics of each element from the data - structure , we propose a hierarchical encoder which relies on two modules .\n",
            " The first layer of the network consists in learning two embedding matrices to embed the record keys and values .\n",
            " R dd are learnt parameters , i ? i , t = 1 , and for all i ?\n",
            " { 1 , ... , I } j ? i , j, t = 1 .\n",
            " 1:T thatare also extracted from y 1:T .\n",
            " \n",
            "Important points in document_classification - 8 :\n",
            "In this paper , we incorporate positioninvariance into RNN and propose a novel model named Disconnected Recurrent Neural Network ( DRNN ) .\n",
            " Another difference to CNN is that DRNN can increase the window size k arbitrarily without increasing the number of parameters .\n",
            " Our contributions can be concluded as follows :\n",
            " incorporate the residual networks into RNN , which makes the model handle longer sequence .\n",
            " The update gate z t decides how much new information is updated .\n",
            " The candidate stateh t is computed b ?\n",
            " Since the output at each step only depends on the previous k ?\n",
            " Here k is the window size , and RN N can be naive RNN , LSTM , GRU or any other kinds of recurrent units .\n",
            " Datasets Introduction\n",
            " We use 7 large - scale text classification datasets which are proposed by .\n",
            " To make these models comparable , we im - plement these models with the same architecture shown in .\n",
            " \n",
            "Important points in document_classification - 5 :\n",
            "Further , the combination of top - level fields and all sub-fields presents current document classification approaches with a combinatorially increasing number of class labels that they can not handle .\n",
            " Researchers have studied and developed a variety of methods for document classification .\n",
            " For this reason many information retrieval systems use decision trees and nave Bayes , methods .\n",
            " The hierarchical classification our methods produce is not only highly accurate but also enables greater understanding of the resulting classification by showing where the document sits within afield or area of study .\n",
            " Nave Bayes calculates where d is the document , resulting in\n",
            " j be the parameter for word j , then\n",
            " Where the counts are indexed by the maximum N - grams .\n",
            " V .\n",
            " In Equation , W rec is the recurrent matrix weight , W in are the input weights , b is the bias , and ?\n",
            " is an element - wise function .\n",
            " The structure of our Hierarchical Deep Learning for Text ( HDLTex ) architecture for each deep learning model is as follows :\n",
            " \n",
            "Important points in document_classification - 7 :\n",
            "By these means , a classifier trained on one language can be transferred to a different one , without the need of resources in that transfer language .\n",
            " The contributions of this work are as follows .\n",
            " Each news story was manually classified into four hierarchical groups : CCAT ( Corporate / Industrial ) , ECAT ( Economics ) , GCAT ( Government / Social ) and MCAT ( Markets ) .\n",
            " Most works in the literature use only 1 000 examples to train the document classifier .\n",
            " It is unlikely that this system will perform well on other languages than the ones used for training or model selection .\n",
            " This type of training is not a cross - lingual approach anymore .\n",
            " A second direction of research is to directly learn multilingual sentence representations .\n",
            " this approach can be found in .\n",
            " \n",
            "Important points in document_classification - 6 :\n",
            "Deep neural networks typically learn a word - level representation for the input text , which is usually a matrix with each row / column as an embedding of a word in the text .\n",
            " As such , the probability of the text belonging to a class is largely determined by their over all matching score regardless of word - level matching signals , which would provide explicit signals for classification ( e.g. , missile strongly indicates the topic of military ) .\n",
            " Hereafter , the interaction layer calculates the matching scores between the words and classes ( i.e. , constructs the interaction matrix ) .\n",
            " Region Embedding\n",
            " The key idea of interaction mechanism is to use the interaction features between the small units ( e.g. , words in the textual contents ) to infer fine - grained clues whether two contents are matching .\n",
            " Different from conventional interaction layer , where the wordlevel representations of both source and target are extracted with encoders like GRU , here we first project classes into real - valued latent representations .\n",
            " However , to keep the simplicity and efficiency of EXAM , here we only use a MLP with two FC layers , where ReLU is employed as the activation function of the first layer .\n",
            " Loss Function\n",
            " We used adam ( Kingma and Ba 2014 ) as the optimizer with the initial learning rate 0.0001 and the batch size is set to 16 .\n",
            " As for the aggregation MLP , we set the size of the hidden layer as 2 times interaction feature length .\n",
            " We separated the dataset into training , validation , and testing with 2,800,000 , 20,000 , and 180,000 questions , respectively .\n",
            " \n",
            "Important points in document_classification - 9 :\n",
            "Here , they form a recursive process to articulate what to be modeled .\n",
            " Improving the efficiency to encode spatial patterns while keeping the flexibility of their representation capability is thus a central issue .\n",
            " More importantly , we show that capsule networks achieves significant improvement when transferring singlelabel to multi-label text classifications over strong baseline methods .\n",
            " Bd be the filter shared in different sliding windows .\n",
            " Adding \" orphan \" category in the text is more effective than in image since there is no single consistent \" background \" object in images , while the stop words are consistent in texts such as predicate \" s \" , \" am \" and pronouns \" his \" , \" she \" .\n",
            " Despite the orphan category in the last capsule layer , we also need a light - weight method between two consecutive layers to route the noise child capsules to extra dimension without any additional parameters and computation consuming .\n",
            " Our dynamic routing algorithm is summarized in Algorithm\n",
            " Suppose W c 1 ? R Ddd and W c 2 ?\n",
            " Baseline methods\n",
            " In the experiments , we evaluate and compare our model with several strong baseline methods including : LSTM / Bi - LSTM , tree - structured LSTM ( Tree - LSTM ) , LSTM regularized by linguistic knowledge ( LR - LSTM ) , CNNrand / CNN - static / CNN - non-static ( Kim , 2014 ) , very deep convolutional network ( VD - CNN ) , and character - level convolutional network ( CL - CNN ) .\n",
            " This should allow the capsule networks to recognize multiple categories in the text even though the model is trained on singlelabel documents .\n",
            " \n",
            "Important points in document_classification - 2 :\n",
            "As the closest comparison point , we find no benefit to the hierarchical modeling proposed by and we are able to achieve good classification results without attention mechanisms .\n",
            " Although this model nicely captures the intuition that modeling word sequences in sentences should be handled separately from sentence - level discourse modeling , one wonders if such complex architectures are really necessary , especially given the size of training data available today .\n",
            " There have been attempts to extend dropout from feedforward neural networks to recurrent ones .\n",
            " 1\n",
            " We choose 512 hidden units for the Bi - LSTM models , whose max - pooled output is regularized using a dropout rate of 0.5 .\n",
            " On all datasets and models , we use 300 - dimensional word vectors pre-trained on Google News .\n",
            " We fail to replicate the reported results of SGM on AAPD using the authors ' codebase and data splits .\n",
            " Baseline Comparison .\n",
            " \n",
            "Important points in document_classification - 19 :\n",
            "RNN can capitalize on distributed representations of words by first converting the tokens comprising each text into vectors , which form a matrix .\n",
            " It is a good choice to utilize 2D convolution and 2D pooling to sample more meaningful features on both the time - step dimension and the feature vector dimension for text classification .\n",
            " This work introduces two combined models BLSTM - 2DPooling and BLSTM - 2DCNN , and verifies them on six text classification tasks , including sentiment analysis , question classification , subjectivity classification , and newsgroups classification .\n",
            " introduced BLSTM with attention mechanism to automatically select features that have a decisive effect on classification .\n",
            " The details of different components are described in the following sections .\n",
            " The main idea is to introduce an adaptive gating mechanism , which decides the degree to keep the previous state and memorize the extracted features of the current data input .\n",
            " The output of the i th word is shown in the following equation :\n",
            " Two-dimensional\n",
            " is an L2 regularization hyper - parameter .\n",
            " Experimental Setup\n",
            " BLSTM - CNN beats all baselines on SST - 1 , SST - 2 , and TREC datasets .\n",
            " \n",
            "Important points in document_classification - 16 :\n",
            "To avoid the problem of gradient exploding or vanishing in the standard RNN , Long Short - term Memory RNN ( LSTM ) and other variants were designed for better remembering and memory accesses .\n",
            " To benefit from the advantages of both CNN and RNN , we design a simple end - to - end , unified architecture by feeding the output of a one - layer CNN into LSTM .\n",
            " Related Work\n",
            " The following two subsections describe how we apply CNN to extract higher - level sequences of word features and LSTM to capture long - term dependencies over window feature sequences respectively .\n",
            " While processing sequential data , it looks at the current input x t as well as the previous output of hidden state h t?1 at each time step .\n",
            " Although many variants of LSTM were proposed , we adopt the standard architecture in this work .\n",
            " For text classification , we regard the output of the hidden state at the last time step of LSTM as the document representation and we add a softmax layer on top .\n",
            " Padding and Word Vector Initialization\n",
            " To benefit from the efficiency of parallel computation of the tensors , we train the model on a GPU .\n",
            " For text preprocessing , we only convert all characters in the dataset to lowercase .\n",
            " For other baseline methods , we compare against SVM with unigram and bigram features , NBoW with average word vector features and paragraph vector that infers the new paragraph vector for unseen documents .\n",
            " \n",
            "Important points in document_classification - 20 :\n",
            "It is difficult to fit a single model for text classification across domains , due to unknown words , specialized context , colloquial language , and other differences between domains .\n",
            " Following , and , we train unsupervised text models on large amounts of unlabelled text data , and transfer the model features to small supervised text problems .\n",
            " Plutchik 's Wheel of Emotions\n",
            " After the first pass , we noticed that random sampling led to some categories being severely under - sampled , below 5 % of tweets .\n",
            " Supervised Finetuning .\n",
            " We also retain the original decoder f d and continue to train it by using language modeling as an auxiliary loss when finetuning on the new corpus .\n",
            " The combined binary problems formulation ( henceforth described as multihead ) allows for a richer error signal that propagates more information through the encoder f e and sentiment representation inf d .\n",
            " For both the multihead MLP and the single linear layer instantiating off d , we found that thresholding predictions produced noticeably better results than using a fixed threshold value such as t * = 0.5 .\n",
            " While the Transformer gets close but does not exceed the state of the art on the SST dataset , it exceeds both the mL - STM and ELMo baseline as well as both Watson and Google Sentiment APIs on the company tweets .\n",
            " This is despite optimally calibrating the API results on the test set .\n",
            " As seen in it appears that the difference between the single and multihead becomes more pronounced for more difficult categories , as well as for smaller dataset sizes .\n",
            " \n",
            "Important points in document_classification - 3 :\n",
            "It is a very different scenario compared to image approaches , where size and speed constrained models have been proposed , .\n",
            " In this paper , we investigate modifications on the network proposed by Conneau et al. with the aim of reducing its number of parameters , storage size and latency with minimal performance degradation .\n",
            " Section VI analyses the results and lastly , Section VII , presents conclusions and direction for future works .\n",
            " A recent tendency in deep models is replacing standard convolutional blocks with Depthwise Separable Convolutions ( DSCs ) .\n",
            " The primary objective is reducing the number of parameters so that the resulting network has a significative lower storage size .\n",
            " The resulting proposed architecture is called Squeezed Very Deep Convolutional Neural Networks ( SVD - CNN ) .\n",
            " Alternatively , a TDSC achieves fewer parameters ( P tdsc ) :\n",
            " presents the standard convolutional block on the left and the proposed convolutional block using TDSC on the right .\n",
            " Considering the network parameters\n",
            " P and assuming that one float number on Cuda environment takes 4 bytes , we can calculate the network storage in megabytes , for all the models , as follows :\n",
            " The CPU inference time obtained by the proposed model was smaller than the base model for the depth 9 ( 25.88 ms against 29 , 13 ms ) and depth 17 ( 47.80 ms against 48.05 ms ) ,\n",
            " \n",
            "Important points in document_classification - 4 :\n",
            "The increasing representation power of the attention mechanism comes with increased model complexity .\n",
            " Further , they suggest that simple models are efficient and interpretable , and have the poten - ar Xiv : 1805.04174v1 [ cs. CL ] 10 May 2018 tial to outperform sophisticated deep neural models .\n",
            " Our label embedding framework has the following salutary properties : ( i ) Label - attentive text representation is informative for the downstream classification task , as it directly learns from a shared joint space , whereas traditional methods proceed in multiple steps by solving intermediate problems .\n",
            " We use for element - wise division when applied to vectors or matrices .\n",
            " f 2 : z ? y , a classifier f 2 annotates the text representation z with a label .\n",
            " They are often pre-trained offline on large corpus , then refined jointly via f 1 and f 2 for task - specific representations .\n",
            " On the other hand , deep hierarchical attention model can improve the pure CNN / RNN models .\n",
            " This is probably due to two reasons : ( i ) the number of classes on these datasets is smaller , and ( ii ) there is no explicit corresponding word embedding available for the label embedding initialization during learning .\n",
            " The compatibility / attention score for the entire text sequence is :\n",
            " where the l - th element of SoftMax is\n",
            " We consider to use each label 's corresponding pre-trained word embeddings as the initialization of the label embeddings .\n",
            " \n",
            "Important points in document_classification - 15 :\n",
            "Traditional adversarial and virtual adversarial training can be interpreted both as a regularization strategy and as defense against an adversary who can supply malicious inputs .\n",
            " This work is the first work we know of to use adversarial and virtual adversarial training to improve a text or RNN model .\n",
            " As a text classification model , we used a simple LSTM - based neural network model , shown in .\n",
            " ( 2 ) with respect to ?.\n",
            " In our experiments , adversarial training refers to minimizing the negative log - likelihood plus L adv with stochastic gradient descent .\n",
            " This approximation corresponds to a 2 ndorder Taylor expansion and a single iteration of the power method on Eq.\n",
            " Regarding pre-processing , we treated any punctuation as spaces .\n",
            " 3\n",
            " A common misconception is that adversarial training is equivalent to training on noisy examples .\n",
            " Noise is actually afar weaker regularizer than adversarial perturbations because , in high dimensional input spaces , an average noise vector is approximately orthogonal to the cost gradient .\n",
            " Dropout ) is a regularization method widely used for many domains including text .\n",
            " \n",
            "Important points in document_classification - 18 :\n",
            "It has been shown that ConvNets can be directly applied to distributed or discrete embedding of words , without any knowledge on the syntactic or semantic structures of a language .\n",
            " The design is modular , where the gradients are obtained by back - propagation to perform optimization .\n",
            " We call each g i ( or h j ) input ( or output ) features , and m ( or n) input ( or output ) feature size .\n",
            " We designed 2 ConvNets - one large and one small .\n",
            " In terms of texts , it is not reasonable to augment the data using signal transformations as done in image or speech recognition , because the exact order of characters may form rigorous syntactic and semantic meaning .\n",
            " We experimented data augmentation by using an English thesaurus , which is obtained from the mytheas component used in LibreOffice 1 project .\n",
            " The classifier used is a multinomial logistic regression in all these models .\n",
            " The inverse document frequency is the logarithm of the division between total number of samples and number of samples with the word in the training subset .\n",
            " Large - scale Datasets and Results\n",
            " Previous research on ConvNets in different areas has shown that they usually work well with largescale datasets , especially when the model takes in low - level raw features like characters in our case .\n",
            " The full dataset contains 600,000 training samples and 130,000 testing samples in each class , whereas the polarity dataset contains 1,800,000 training samples and 200,000 testing samples in each polarity sentiment .\n",
            " \n",
            "Important points in document_classification - 17 :\n",
            "However , it is less clear how we should best represent a sequence of words , e.g. a whole sentence , which has complicated syntactic and semantic relations .\n",
            " It is well known that a fully connected one hidden layer neural network can in principle learn any realvalued function , but much better results can be obtained with a deep problem - specific architecture which develops hierarchical representations .\n",
            " There have been previous attempts to use ConvNets for text processing .\n",
            " If not otherwise stated , all approaches operate on words which are projected into a high - dimensional space .\n",
            " Architecture\n",
            " sis fixed to 1024 , and f 0 can be seen as the \" RGB \" dimension of the input text .\n",
            " Most of the previous applications of ConvNets to NLP use an architecture which is rather shallow ( up to 6 convolutional layers ) and combines convolutions of different sizes , e.g. spanning 3 , 5 and 7 tokens .\n",
            " We have also investigated the same kind of \" ResNet shortcut \" connections as in , namely identity and 1 1 convolutions ( see ) .\n",
            " The significant improvements which we obtain on all data sets compared to Zhang 's convolutional models do not include any data augmentation technique .\n",
            " Max - pooling performs better than other pooling types .\n",
            " It should be also noted that some of the tasks are very ambiguous , in particular sentiment analysis for which it is difficult to clearly associate fine grained labels .\n",
            " \n",
            "Important points in document_classification - 14 :\n",
            "where x is a concatenation of one - hot vectors ( therefore , ' one - hot ' in the name ) of the words in the - th region ( of a fixed size ) , and the weight matrix W and the bias vector b need to be trained .\n",
            " This enables a larger region , but at the expense of losing word order in the region and so its use maybe limited .\n",
            " Our strategy is to simplify the model as much as possible , including elimination of a word embedding layer routinely used to produce input to LSTM .\n",
            " The centerpiece of LSTM is the memory cells ct , designed to counteract the risk of vanishing / exploding gradients , thus enabling learning of dependencies over larger time lags than feasible with traditional recurrent networks .\n",
            " Altogether , elimination of the word embedding layer was found to be useful ; thus , we base our work on one - hot LSTM .\n",
            " Pooling : simplifying sub - problems\n",
            " At the time of training , we can chop each document into segments of a fixed length that is sufficiently long ( e.g. , 50 or 100 ) and process all the segments in a mini batch in parallel as if these segments were individual documents .\n",
            " However , we found that gains from overlapping segments tend to be small and so our experiments reported below were done without overlapping .\n",
            " In , on three out of the four datasets , oh - 2 LSTMp outperforms SVM and the CNN .\n",
            " However , on RCV1 , it underperforms both .\n",
            " We use the tv-embeddings obtained from unlabeled data to produce additional input to LSTM by replacing and ( 3 ) by the following :\n",
            " \n",
            "Important points in document_classification - 11 :\n",
            "illustrates this problem with the classification task as an example .\n",
            " In this paper , we propose a task - oriented word embedding method ( denoted as ToWE ) to solve the aforementioned problem .\n",
            " In our method , the words ' contextual information and task information are inherently jointed to construct the word embeddings .\n",
            " To meet the needs of real - world applications , rational word embeddings should have the ability to capture both the semantics of words and the task - specific features of words .\n",
            " Then the correlations among these words are used to model the functional features in the embedding space .\n",
            " Formally , we design the following formula to measure the importance of word w to the k -th category as a salient word :\n",
            " where ?\n",
            " The context - aware component and the function - aware component are jointly optimized , so we then obtain the following object function :\n",
            " ( 5 ) The SST 6 dataset contains the movie reviews in the Stanford Sentiment Treebank labeled by comprising one sentence for each review .\n",
            " 50 % of the MR and SST datasets are partitioned randomly into the training set and 50 % into the test set .\n",
            " ( 1 ) Our method performs better than the other methods , and are proved to be highly reliable for the text classification task .\n",
            " \n",
            "Important points in document_classification - 1 :\n",
            "Transfer methods can explicitly rely on machine translation models built from such parallel corpora .\n",
            " However , in real world applications , we must also bridge the domain gap across different languages , as well as the language gap .\n",
            " In particular , we focus on two approaches for domain adaptation .\n",
            " In such cases , it is known that further pre-training the language model on in - domain text is helpful ; ) .\n",
            " We refer to this approach as the MLM pre-training .\n",
            " Unsupervised Data Augmentation\n",
            " The results of these comparisons are included in out detailed ablation study in the experiments section .\n",
            " ( 1 ) , one can see that the data x that feed to model in the training phrase is sampled from three domains : ( 1 ) the source domain P src ( x ) , ( 2 ) the target domain P tgt ( x ) and the augmented sample domain P aug ( x ) .\n",
            " The pre-processing scripts for the above datasets , augmented samples and experiment settings needed to reproduce results are released in the Github repo 1 .\n",
            " MASKED LANGUAGE MODEL PRE - TRAINING STRATEGY\n",
            " Finally , comparing with the best cross - lingual results and monolingual fine - tune baseline , we are able to completely close the performance gap by utilizing unlabeled data in the target language .\n",
            " \n",
            "Important points in document_classification - 12 :\n",
            "We construct a single large graph from an entire corpus , which contains words and documents as nodes .\n",
            " We propose a novel graph neural network method for text classification .\n",
            " There are also existing studies on converting texts to graphs and perform feature engineering on graphs and subgraphs .\n",
            " A GCN ( Kipf and Welling 2017 ) is a multilayer neural network that operates directly on a graph and induces embedding vectors of nodes based on properties of their neighborhoods .\n",
            " Formally , the weight of edge between node i and node j is defined as\n",
            " A positive PMI value implies a high semantic correlation of words in a corpus , while a negative PMI value indicates little or no semantic correlation in the corpus .\n",
            " Thus although there is no direct document - document edges in the graph , the two - layer GCN allows the information exchange between pairs of documents .\n",
            " In this section we evaluate our Text Graph Convolutional Networks ( Text GCN ) on two experimental tasks .\n",
            " As we focus on single - label text classification , the documents belonging to multiple categories are excluded so that 7,400 documents belonging to only one category remain .\n",
            " 3,357 documents are in the training set and 4,043 documents are in the test set . R52 and R8 3 ( all - terms version ) are two subsets of the Reuters 21578 dataset .\n",
            " Nevertheless , CNN and LSTM rely on pre-trained word embeddings from external corpora while Text GCN only uses information in the target input corpus .\n",
            " \n",
            "Important points in document_classification - 13 :\n",
            "However , in , very shallow 1 - layer word - level CNNs were shown to be more accurate and much faster than the very deep characterlevel CNNs of .\n",
            " After converting discrete text to continuous representation , the DPCNN architecture simply alternates a convolution block and a downsampling layer over and over 1 , leading to a deep network in which internal data size ( as well as per-layer computation ) shrinks in a pyramid shape .\n",
            " It is followed by stacking of convolution blocks ( two convolution layers and a shortcut ) interleaved with pooling layers with stride 2 for downsampling .\n",
            " Here , pre-activation refers to activation being done before weighting instead of after as is typically done .\n",
            " Beyond that , the optimum input type and the optimum region size can only be determined empirically .\n",
            " The n-gram input turned out to be prone to overfitting in the supervised setting , likely due to its high representation power , but it is very useful as the input to unsupervised embeddings , which we discuss next .\n",
            " We will show below that use of unsupervised embeddings in this way consistently improves the accuracy of DPCNN .\n",
            " Data and data preprocessing\n",
            " [ JZ16 ] : Johnson and Zhang bags of { 1,2,3 } - grams , retaining only the most frequent 30K words or 200K { 1,2,3 } - grams .\n",
            " Training was done on the labeled data ( disregarding the labels ) , setting the training objectives to the prediction of adjacent regions of the same size as the input region ( i.e. , 5 or 9 ) .\n",
            " 4 .\n",
            " \n",
            "Important points in document_classification - 0 :\n",
            "The first weight matrix A is a look - up table over the words .\n",
            " This model is trained asynchronously on multiple CPUs using stochastic gradient descent and a linearly decaying learning rate .\n",
            " If the node is at depth l + 1 with parents n 1 , . . . , n l , it s probability is\n",
            " Note that we can increase the accuracy slightly by using more n-grams , for example with trigrams , the performance on Sogou goes up to 97.1 % .\n",
            " Unlike unsupervisedly trained word vectors from word2vec , our word features can be averaged together to form good sentence representations .\n",
            " We will publish our code so that the research community can easily build on top of our work .\n",
            " \n",
            "Important points in document_classification - 10 :\n",
            "The weights are computed using a novel neural attention mechanism that enables the model to focus on a small subset of the entities that are less ambiguous in meaning and more relevant to the document .\n",
            " Given a document , our model addresses the text classification task by using the following two steps : it first detects entities from the document , and then classifies the document using the proposed model with the detected entities as inputs .\n",
            " We selected these systems because they are capable of detecting non-named entities ( e.g. , technical terms ) that are useful for addressing the text classification task .\n",
            " Pretrained Embeddings\n",
            " FTS- BRNN\n",
            " NTEE This model is a state - of - the - art model that uses a multi - layer perceptron classifier with the features computed using the embeddings of words and entities trained on Wikipedia using the neural network model proposed in their paper .\n",
            " These results clearly highlighted the effectiveness of our approach , which addresses text classification by using a small number of unambiguous and relevant entities detected by the proposed attention mechanism .\n",
            " Analysis\n",
            " This is because these questions tended to provide indirect clues ( e.g. , describing a notable person born in the country ) and most entities used in these clues do not directly indicate the answer ( i.e. , country names ) .\n",
            " Furthermore , our model failed in difficult cases such as predicting Tokugawa shogunate instead of Tokugawa Ieyasu .\n",
            " \n",
            "Important points in constituency_parsing - 8 :\n",
            "In Section 5.1 , we demonstrate that a simple scheme based on concatenating character embeddings of word prefixes / suffixes can outperform using part - of - speech tags from an external system .\n",
            " Code and trained English models are publicly available .\n",
            " Our parser assigns a real - valued score s ( T ) to each tree T , which decomposes as\n",
            " The first sublayer in each of our 8 layers is a multi-headed self - attention mechanism , which is the only means by which information may propagate between positions in the sentence .\n",
            " Details regarding hyperparameter choice and optimizer settings are presented in Appendix A.\n",
            " y k and the odd coordinates contribute to ?\n",
            " The defining feature of our encoder is the use of self - attention , which is the only mechanism for transfer of information between different locations throughout a sentence .\n",
            " Development - set F1 scores when attention is constrained to not exceed a particular distance in the sentence at test time only .\n",
            " Our approach ( CHARCONCAT ) is inspired by , who found it effective to replace words with frequently - occurring suffixes , and the observation that our original tag embeddings are rather high - dimensional .\n",
            " To represent a word , we extract its first 8 letters and last 8 letters , embed each letter , and concatenate the results .\n",
            " Conclusion\n",
            " \n",
            "Important points in constituency_parsing - 2 :\n",
            "s 2s baseline parser\n",
            " Since a one - to - one mapping exists between a parse tree and it s linearized form ( if the linearized form is a valid tree ) , we can recover parse trees from the predicted linearized parse tree .\n",
            " Its primary advantage is a significant reduction of the serious out - of - vocabulary ( OOV ) problem .\n",
            " Multi - task learning\n",
            " Following their success , we also trained RNN - LMs on the PTB dataset with their published preprocessing code 8 to reproduce the experiments in Choe and Charniak ( 2016 ) for our LM-rerank .\n",
            " Our experiments used the English Penn Treebank data , which are the most widely used benchmark data in the literature .\n",
            " For LM - reranking , we first generated 80 candidates by the above eight ensemble models and selected the best parse tree for each input in terms of the LM- reranker .\n",
            " The following observations appear informative for building strong baseline systems :\n",
            " \n",
            "Important points in constituency_parsing - 1 :\n",
            "These improvements are consistent with , if slightly behind , those achieved by the concurrently developed BERT pretraining approach , which we will discuss in more detail in the next section .\n",
            " Our work was inspired by ELMo and the generative pretraining ( GPT ) approach of .\n",
            " In comparison , we optimize a single loss function that requires the model to predict each token of an input sentence given all surrounding tokens .\n",
            " The output of the FFN block is projected into V classes representing the types in the vocabulary .\n",
            " For named entity recognition and parsing we use task - specific architectures which we fine - tune together with the language model but with different learning rate .\n",
            " No Masking .\n",
            " We use a cosine schedule to linearly warm up the learning rate from 1e - 07 to the target value over the first 10 % of training steps , and then anneal the learning rate to 1e - 06 , following the cosine curve for the remaining steps .\n",
            " Experimental setup\n",
            " The Semantic Textual Similarity Benchmark requires predicting a similarity score between 1 and 5 for a sentence pair ; we report the Spearman correlation coefficient ( scc ) .\n",
            " Finally , there are four natural laguage inference tasks : the Multi - Genre Natural Language Inference ( MNLI ; , the Stanford Question Answering Dataset ( QNLI ; , the Recognizing Textual Entailment ( RTE ; .\n",
            " Domain and amount of training data\n",
            " \n",
            "Important points in constituency_parsing - 7 :\n",
            "Based on the findings , we augment the RNNG composition function with a novel gated attention mechanism ( leading to the GA - RNNG ) to incorporate more interpretability into the model in 4 .\n",
            " Recurrent Neural Network Grammars\n",
            " To generate a sentence x and its phrase - structure tree y , the RNNG samples a sequence of actions to construct y top - down .\n",
            " Beyond this , Choe and Charniak 's generative variant of is another instance where generative models trained on the PTB outperform discriminative models .\n",
            " This strongly supports the importance of the composition function : a proper REDUCE operation that transforms a constituent 's parts and nonterminal label into a single explicit ( vector ) representation is helpful to performance .\n",
            " A similar performance degradation is seen in language modeling : the stack - only RNNG achieves the best performance , and ablating the stack is most harmful .\n",
            " Do the phrasal representations learned by RN - NGs depend on individual lexical heads or multiple heads ?\n",
            " Gated Attention Composition\n",
            " ( in blue ) shows much less than 2 average \" choices \" across nonterminal categories , which also holds true for all other categories not shown .\n",
            " For comparison we also report the average perplexity of the uniform distribution for the same nonterminal categories ; this represents the highest entropy cases where there is no headedness at all by assigning the same attention weight to each constituent ( e.g. attention weights of 0.25 each for phrases with four constituents ) .\n",
            " The Role of Nonterminal Labels\n",
            " \n",
            "Important points in constituency_parsing - 5 :\n",
            "Given the sentence in shift - reduce parser takes the action sequence in ( b ) to build the output , where an S is first made and then an NP is generated .\n",
            " Details of the action system are introduced in Section 2.1 , Section 2.2 and Section 3 , respectively .\n",
            " With the Sand the word \" likes \" , the system projects an VP , which can serve as top - down guidance .\n",
            " The bottom - up parser can be summarized as the deductive system in ( a ) .\n",
            " For example , given the tree \" ( S ab c d ) \" , the traversal is \" a Sb c d \" if k = 1 while the traversal is \" a b Sc d \" if k = 2 .\n",
            " Note that the top - down parser can be regarded as a special case of a generalized version of the in - order parser with k = 0 , and the bottom - up parser can be regarded as a special case with k = ?.\n",
            " Greedy action classification\n",
            " x is the word representation , and the representation of action history h ah is :\n",
            " Results\n",
            " Model\n",
            " ...\n",
            " \n",
            "Important points in constituency_parsing - 3 :\n",
            "We suspected that the attention model of Bahdanau et al.\n",
            " LSTM + A Parsing Model\n",
            " The above equation assumes a deep LSTM whose input sequence is x = ( A 1 , . . . , A TA , B 1 , . . . , B TB ) , so ht denotes t- th element of the h-sequence of topmost LSTM .\n",
            " As described below , we use 3 LSTM layers , reverse the input sentence and normalize part - of - speech tags .\n",
            " 3 Experiments\n",
            " For one , we trained on the standard WSJ training dataset .\n",
            " We call the set of ? 11 million sentences selected in this way , together with the ? 90 K golden sentences described above , the high - confidence corpus .\n",
            " We use the standard EVALB tool 2 for evaluation and report F 1 scores on our developments set ( section 22 of the Penn Treebank ) and the final test set ( section 23 ) in .\n",
            " A model only achieved an F 1 score of 86.5 on our development set , that is over 2 points lower than the 88.7 of a LSTM + A+D model .\n",
            " Pre-training influence .\n",
            " partially alleviate the heavy reliance on manual modeling of linguistic structure by using latent variables to learn a more articulated model .\n",
            " \n",
            "Important points in constituency_parsing - 6 :\n",
            "We have reduced parsing to language modeling and can use language modeling techniques of estimating P ( z t | z 1 , , z t?1 ) for parsing .\n",
            " Wis a parameter matrix and [ i ] indexes ith element of a vector .\n",
            " Finally the decoder predicts y t given ht .\n",
            " 3\n",
            " at Brown University for setting up GPU machines and David McClosky for helping us train Charniak parser on millions trees .\n",
            " \n",
            "Important points in constituency_parsing - 4 :\n",
            "Perhaps generative models\n",
            " A shows gains merely from subtle model combination effects .\n",
            " Our findings suggest the presence of model combination effects in both generative parsers : when parses found by searching directly in the generative parser are added to a list of candidates from a strong base parser ( the RNNG discriminative parser , RD ) , performance decreases when compared to using just candidates from the base parser , i.e. , B ? A ?\n",
            " This search failure produces very low evaluation performance : with a beam of size K = 100 , action - synchronous beam search achieves 29.1 F1 for RG and 27.4 F1 for LM on the development set .\n",
            " Augmenting the candidate set\n",
            " A , where we search in B to get a set of candidate parses for each sentence , and This does seem to be the case for both generative models , as shown in , which presents F 1 scores on the development set when varying the models used to produce the candidates and to score them .\n",
            " Taken together , these results suggest that model combination contributes to the success of both models , but to a larger extent for RG .\n",
            " These results are given in columns RD + RG and RD + LM in .\n",
            " This provides a concrete improvement for these particular generative reranking procedures for parsing .\n",
            " More generally , it supports the idea that hybrid systems , which rely on one model to produce a set of candidates and another to determine which candidates are good , should explore combining their scores and candidates when possible .\n",
            " \n",
            "Important points in constituency_parsing - 0 :\n",
            "time deterministic parser ( provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed ) ; however , our algorithm generates arbitrary tree structures directly , without the binarization required by shift - reduce parsers .\n",
            " Formally , an RNNG is a triple ( N , ? , ? ) consisting of a finite set of nonterminal symbols ( N ) , a finite set of terminal symbols ( ? ) such that N ? ? = ? , and a collection of neural network parameters ?.\n",
            " The parsing algorithm transforms a sequence of words x into a parse tree y using two data structures ( a stack and an input buffer ) .\n",
            " To designate the set of valid parser transitions , we write AD ( B , S , n ) .\n",
            " A detailed analysis of the algorithmic properties of our top - down parser is beyond the scope of this paper ; however , we briefly state several facts .\n",
            " The top of the stack is to the right , and the buffer is consumed from left to right .\n",
            " RNNGs use the generator transition set just presented to define a joint distribution on syntax trees ( y ) and words ( x ) .\n",
            " Refer to for an illustration of the architecture .\n",
            " Importance Sampling\n",
            " Our generative model p ( x , y) defines a joint distribution on trees ( y ) and sequences of words ( x ) .\n",
            " Our language model combines work from two modeling traditions : ( i ) recurrent neural network language models and ( ii ) syntactic language modeling .\n",
            " "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "df=pd.DataFrame(columns=['Dataset Name', 'Total Imp Lines','Total Lines','Avg Imp Lines','Avg total Lines','Imp Lines %'])\n",
        "\n",
        "s=\"/\"\n",
        "path = r\"/content/drive/My Drive/data/test-data-master/test-data-master\"\n",
        "for file in os.listdir(path):\n",
        "    path2 = r\"/content/drive/My Drive/data/test-data-master/test-data-master\"+s+file\n",
        "    imp=0\n",
        "    tot=0\n",
        "    c=0\n",
        "    for file2 in os.listdir(path2):\n",
        "        path3 = path2+s+file2\n",
        "        c+=1\n",
        "        for file3 in os.listdir(path3):\n",
        "            #print(file3)\n",
        "            if(file3==\"sentences.txt\"):\n",
        "                with open(path3+s+\"sentences.txt\", 'r') as fse:\n",
        "                    impsen = len(fse.readlines())\n",
        "                    imp+=impsen\n",
        "            if(len(file3)>11 and file3[-14:]==\"Stanza-out.txt\"):\n",
        "                with open(path3+s+file3, 'r') as fse:\n",
        "                    totsen = len(fse.readlines())\n",
        "                    tot+=totsen\n",
        "    df.loc[len(df.index)]=[file,imp,tot,imp/c,tot/c,(imp/tot)*100]\n",
        "print(df)\n",
        "df.to_csv('C:\\harshita\\Mini Project\\drive-download-20220510T155820Z-001\\output.csv',index=False)"
      ],
      "metadata": {
        "id": "RIzT4PveD3Vd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86a49763-a4c5-4be1-abc9-fd673097b121"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Dataset Name Total Imp Lines Total Lines  Avg Imp Lines  \\\n",
            "0  natural_language_inference             558        6759      17.437500   \n",
            "1              face_alignment             425        5168      22.368421   \n",
            "2      coreference_resolution             168        1784      16.800000   \n",
            "3              face_detection             440        5535      20.000000   \n",
            "4              entity_linking             303        3598      17.823529   \n",
            "5          hypernym_discovery              99        1226      11.000000   \n",
            "6          dependency_parsing             119        1933      13.222222   \n",
            "7     data-to-text_generation             100        1616      14.285714   \n",
            "8     document_classification             385        4529      18.333333   \n",
            "9        constituency_parsing             123        1652      13.666667   \n",
            "\n",
            "   Avg total Lines  Imp Lines %  \n",
            "0       211.218750     8.255659  \n",
            "1       272.000000     8.223684  \n",
            "2       178.400000     9.417040  \n",
            "3       251.590909     7.949413  \n",
            "4       211.647059     8.421345  \n",
            "5       136.222222     8.075041  \n",
            "6       214.777778     6.156234  \n",
            "7       230.857143     6.188119  \n",
            "8       215.666667     8.500773  \n",
            "9       183.555556     7.445521  \n"
          ]
        }
      ]
    }
  ]
}